{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a3598f-87ee-4388-af0f-f77827002d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nconda install ipykernel -y\\npython -m ipykernel install --user --name sedd_env --display-name \"Python (sedd_env)\"\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cd Score-Entropy-Discrete-Diffusion\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-py310_23.11.0-2-Linux-x86_64.sh -O Miniconda3.sh\n",
    "bash Miniconda3.sh -b -p $HOME/miniconda\n",
    "source $HOME/miniconda/etc/profile.d/conda.sh\n",
    "'''\n",
    "# environment.yml 의존성 문제로 파일 내용 변경됨\n",
    "# 아래 내용 터미널에서 실행\n",
    "'''\n",
    "conda env create -f environment.yml\n",
    "conda activate sedd\n",
    "'''\n",
    "# (sedd) 환경에서 실행\n",
    "'''\n",
    "pip install \\\n",
    "  accelerate==0.27.2 \\\n",
    "  aiohttp==3.9.3 \\\n",
    "  aiosignal==1.3.1 \\\n",
    "  antlr4-python3-runtime==4.9.3 \\\n",
    "  appdirs==1.4.4 \\\n",
    "  async-timeout==4.0.3 \\\n",
    "  attrs==23.2.0 \\\n",
    "  beartype==0.14.1 \\\n",
    "  better-abc==0.0.3 \\\n",
    "  certifi==2022.12.7 \\\n",
    "  charset-normalizer==2.1.1 \\\n",
    "  click==8.1.7 \\\n",
    "  cloudpickle==3.0.0 \\\n",
    "  cmake==3.25.0 \\\n",
    "  datasets==2.17.1 \\\n",
    "  dill==0.3.8 \\\n",
    "  docker-pycreds==0.4.0 \\\n",
    "  einops==0.7.0 \\\n",
    "  fancy-einsum==0.0.3 \\\n",
    "  filelock==3.9.0 \\\n",
    "  frozenlist==1.4.1 \\\n",
    "  fsspec==2023.10.0 \\\n",
    "  gitdb==4.0.11 \\\n",
    "  gitpython==3.1.42 \\\n",
    "  huggingface-hub==0.21.1 \\\n",
    "  hydra-core==1.3.2 \\\n",
    "  hydra-submitit-launcher==1.2.0 \\\n",
    "  idna==3.4 \\\n",
    "  jaxtyping==0.2.25 \\\n",
    "  jinja2==3.1.2 \\\n",
    "  lit==15.0.7 \\\n",
    "  markdown-it-py==3.0.0 \\\n",
    "  markupsafe==2.1.3 \\\n",
    "  mdurl==0.1.2 \\\n",
    "  mpmath==1.3.0 \\\n",
    "  multidict==6.0.5 \\\n",
    "  multiprocess==0.70.16 \\\n",
    "  networkx==3.2.1 \\\n",
    "  ninja==1.11.1.1 \\\n",
    "  numpy==1.24.1 \\\n",
    "  omegaconf==2.3.0 \\\n",
    "  pandas==2.2.1 \\\n",
    "  pillow==10.2.0 \\\n",
    "  protobuf==4.25.3 \\\n",
    "  psutil==5.9.8 \\\n",
    "  pyarrow==15.0.0 \\\n",
    "  pyarrow-hotfix==0.6 \\\n",
    "  pygments==2.17.2 \\\n",
    "  python-dateutil==2.8.2 \\\n",
    "  pytz==2024.1 \\\n",
    "  pyyaml==6.0.1 \\\n",
    "  regex==2023.12.25 \\\n",
    "  requests==2.28.1 \\\n",
    "  rich==13.7.0 \\\n",
    "  safetensors==0.4.2 \\\n",
    "  sentry-sdk==1.40.6 \\\n",
    "  setproctitle==1.3.3 \\\n",
    "  smmap==5.0.1 \\\n",
    "  submitit==1.5.1 \\\n",
    "  sympy==1.12 \\\n",
    "  tokenizers==0.15.2 \\\n",
    "  tqdm==4.66.2 \\\n",
    "  transformer-lens==1.14.0 \\\n",
    "  transformers==4.38.1 \\\n",
    "  triton==2.0.0 \\\n",
    "  typeguard==2.13.3 \\\n",
    "  typing-extensions==4.8.0 \\\n",
    "  tzdata==2024.1 \\\n",
    "  urllib3==1.26.13 \\\n",
    "  wandb==0.16.3 \\\n",
    "  xxhash==3.4.1 \\\n",
    "  yarl==1.9.4\n",
    "pip install flash-attn==2.2.2\n",
    "'''\n",
    "'''\n",
    "conda install ipykernel -y\n",
    "python -m ipykernel install --user --name sedd_env --display-name \"Python (sedd_env)\"\n",
    "'''\n",
    "# 주피터노트북 커널 sedd_env 로 변경 후 아래 셀들 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd8001b-4a22-4b05-8dbb-9505a13f77ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial working directory: /workspace/Score-Entropy-Discrete-Diffusion\n",
      "라이브러리 임포트 및 경로 설정 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/sedd/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 셀 1: 필요한 라이브러리 임포트 및 경로 설정\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 클론한 레포지토리의 경로를 sys.path에 추가\n",
    "# 현재 Notebook 파일(.ipynb)이 Score-Entropy-Discrete-Diffusion 폴더 내에 있다면\n",
    "# 그리고 터미널에서 해당 폴더로 이동 후 jupyter notebook을 실행했다면,\n",
    "# 아래 os.chdir 부분은 필요 없을 수 있습니다.\n",
    "# 현재 작업 디렉토리를 확인하고, 레포지토리 루트 디렉토리로 설정합니다.\n",
    "print(f\"Initial working directory: {os.getcwd()}\")\n",
    "if not os.path.basename(os.getcwd()) == 'Score-Entropy-Discrete-Diffusion':\n",
    "    # 만약 현재 디렉토리가 레포지토리 루트가 아니라면,\n",
    "    # 사용자님의 실제 클론 경로에 맞게 수정해야 할 수 있습니다.\n",
    "    # 예: /workspace/Score-Entropy-Discrete-Diffusion\n",
    "    # 이 코드는 현재 스크립트가 있는 위치를 기준으로 상위 디렉토리로 이동하는 방식은 아닙니다.\n",
    "    # 가장 확실한 방법은 절대 경로를 사용하거나,\n",
    "    # Jupyter Notebook을 Score-Entropy-Discrete-Diffusion 폴더 내에서 실행하는 것입니다.\n",
    "    # 아래는 일반적인 RunPod 환경을 가정한 예시 경로입니다.\n",
    "    repo_path = '/workspace/Score-Entropy-Discrete-Diffusion'\n",
    "    if os.path.exists(repo_path) and os.path.isdir(repo_path):\n",
    "        os.chdir(repo_path)\n",
    "        print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(f\"Warning: Repository path '{repo_path}' not found. Make sure model files are accessible.\")\n",
    "\n",
    "# sys.path에 현재 디렉토리 (레포지토리 루트) 추가\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import torch\n",
    "# load_model.py가 현재 작업 디렉토리에 있으므로 바로 임포트 가능\n",
    "from load_model import load_model\n",
    "\n",
    "print(\"라이브러리 임포트 및 경로 설정 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2989f1-5224-4e11-81bb-16390a30a8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model from: louaaron/sedd-small\n",
      "\n",
      "Model loaded successfully!\n",
      "Model type: <class 'model.transformer.SEDD'>\n",
      "Graph type: <class 'graph_lib.Absorbing'>\n",
      "Noise type: <class 'noise_lib.LogLinearNoise'>\n"
     ]
    }
   ],
   "source": [
    "# 셀 2: SEDD 모델 로드\n",
    "try:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Hugging Face Hub에서 모델 로드 시도\n",
    "    model_path = \"louaaron/sedd-small\"\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "\n",
    "    # load_model.py의 load_model 함수 호출 (내부적으로 load_model_hf 우선 시도)\n",
    "    score_model, graph, noise = load_model(model_path, device)\n",
    "\n",
    "    print(\"\\nModel loaded successfully!\")\n",
    "    print(f\"Model type: {type(score_model)}\")\n",
    "    # score_model이 DDP로 감싸져 있을 수 있으므로 .module로 실제 모델 접근\n",
    "    if hasattr(score_model, 'module'):\n",
    "        print(f\"Actual model type (inside DDP or similar): {type(score_model.module)}\")\n",
    "    print(f\"Graph type: {type(graph)}\")\n",
    "    print(f\"Noise type: {type(noise)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model loading: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08db012b-8961-4648-8cf7-75e2d31195a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일 로드 완료.\n",
      "order_df shape: (127, 5)\n",
      "machine_df shape: (21151, 3)\n",
      "\n",
      "총 127개의 주문 데이터 전처리 완료.\n",
      "\n",
      "--- 전처리된 데이터 샘플 (첫 2개) ---\n",
      "\n",
      "--- 샘플 1 ---\n",
      "Input Text Components: {'<DUE>': '2021-05-13', '<ITEM>': 'K04033', '<COST>': '25870', '<QTY>': '318', '<URGENT>': '1'}\n",
      "Conditioning Vector: {'K04033': {'404': 12.84, '405': 10.42, '407': 8.69, '408': 9.12, '409': 10.33, '410': 8.99, '412': 12.32, '416': 11.23, '422': 4.0, '424': 9.87, '426': 11.77, '433': 9.54, '434': 13.05, '435': 9.0, '436': 9.0, '438': 4.4, '439': 12.47, '440': 9.4}}\n",
      "Target Vector: {'output_text': '더미 GT 결과: 생산 계획 최적화 완료.'}\n",
      "\n",
      "--- 샘플 2 ---\n",
      "Input Text Components: {'<DUE>': '2021-05-24', '<ITEM>': 'K04031', '<COST>': '16229', '<QTY>': '383', '<URGENT>': '1'}\n",
      "Conditioning Vector: {'K04031': {'407': 4.85, '408': 3.83, '409': 5.37, '410': 3.09, '416': 2.73, '424': 3.94, '425': 5.47, '426': 6.25, '433': 3.37, '434': 3.86, '435': 3.8, '436': 4.53, '440': 3.85}}\n",
      "Target Vector: {'output_text': '더미 GT 결과: 생산 계획 최적화 완료.'}\n"
     ]
    }
   ],
   "source": [
    "# 셀 3: 데이터 로드 및 전처리\n",
    "import pandas as pd\n",
    "import numpy as np # NaN 비교 등을 위해 사용\n",
    "\n",
    "# --- 데이터 파일 경로 ---\n",
    "# 실제 파일 경로로 수정해주세요.\n",
    "# 예시: 현재 디렉토리에 있다면 그대로 사용\n",
    "order_info_path = 'sampledata/order_info.csv'\n",
    "machine_info_path = 'sampledata/machine_info.csv' # 이전에 machine_info_sample.csv를 사용하셨다면, 실제 파일로 변경\n",
    "\n",
    "# --- 데이터 로드 ---\n",
    "try:\n",
    "    order_df = pd.read_csv(order_info_path)\n",
    "    machine_df = pd.read_csv(machine_info_path)\n",
    "    print(\"CSV 파일 로드 완료.\")\n",
    "    print(f\"order_df shape: {order_df.shape}\")\n",
    "    print(f\"machine_df shape: {machine_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: CSV 파일을 찾을 수 없습니다. 경로를 확인하세요: {order_info_path} 또는 {machine_info_path}\")\n",
    "    # 이 경우, 이후 코드 실행이 어려우므로 중단하거나 파일 경로를 수정해야 합니다.\n",
    "    order_df = pd.DataFrame() # 빈 DataFrame으로 초기화하여 에러 방지 (실제 사용시에는 파일 필요)\n",
    "    machine_df = pd.DataFrame()\n",
    "\n",
    "# --- 전처리 함수 정의 ---\n",
    "\n",
    "def preprocess_urgent(value):\n",
    "    \"\"\" '선급' 컬럼 값을 URGENT 값으로 매핑 \"\"\"\n",
    "    if pd.isna(value) or value == '': # NaN 또는 빈 문자열인 경우\n",
    "        return 0\n",
    "    elif value == '검사품':\n",
    "        return 1\n",
    "    else: # 그 외 모든 문자열 값\n",
    "        return 2\n",
    "\n",
    "def create_input_text_components(row):\n",
    "    \"\"\" DataFrame row에서 input_text 구성요소를 생성 \"\"\"\n",
    "    due_date = row['영업납기']\n",
    "    item_code = row['중산도면']\n",
    "    cost = int(row['단가']) # 정수형으로 변환\n",
    "    qty = int(row['수량'])   # 정수형으로 변환\n",
    "    urgent_value = preprocess_urgent(row['선급'])\n",
    "\n",
    "    # <URGENT> 태그를 포함한 구조화된 텍스트의 구성요소 딕셔너리\n",
    "    # 나중에 이 딕셔너리를 바탕으로 실제 <TAG>VALUE 형태의 문자열을 만듭니다.\n",
    "    components = {\n",
    "        \"<DUE>\": str(due_date),\n",
    "        \"<ITEM>\": str(item_code),\n",
    "        \"<COST>\": str(cost),\n",
    "        \"<QTY>\": str(qty),\n",
    "        \"<URGENT>\": str(urgent_value)\n",
    "    }\n",
    "    return components\n",
    "\n",
    "def create_conditioning_vector(item_code, machine_df):\n",
    "    \"\"\" 특정 item_code에 대한 conditioning_vector 생성 \"\"\"\n",
    "    # machine_df에서 해당 item_code를 가진 machine 정보 필터링\n",
    "    relevant_machines = machine_df[machine_df['item'] == item_code]\n",
    "    \n",
    "    condition_dict = {}\n",
    "    if not relevant_machines.empty:\n",
    "        # machine_df의 'machine'은 float으로 읽힐 수 있으므로 str으로 변환 고려\n",
    "        # 또한, 'capacity'도 숫자형이어야 함\n",
    "        condition_dict[str(item_code)] = {\n",
    "            str(int(m_row['machine'])) if pd.notna(m_row['machine']) else str(m_row['machine']): float(m_row['capacity'])\n",
    "            for _, m_row in relevant_machines.iterrows()\n",
    "        }\n",
    "    else:\n",
    "        # 해당 item_code에 대한 기계 정보가 없으면 빈 딕셔너리 또는 특정 표시\n",
    "        condition_dict[str(item_code)] = {} \n",
    "        \n",
    "    return condition_dict\n",
    "\n",
    "def create_target_vector():\n",
    "    \"\"\" 더미 target_vector 생성 \"\"\"\n",
    "    # 현재는 간단한 더미 텍스트로 설정\n",
    "    # 추후 실제 GT 형식에 맞게 수정 필요\n",
    "    return {\"output_text\": \"더미 GT 결과: 생산 계획 최적화 완료.\"}\n",
    "\n",
    "\n",
    "# --- 최종 데이터 구조 생성 ---\n",
    "processed_data_list = []\n",
    "\n",
    "if not order_df.empty:\n",
    "    for index, row in order_df.iterrows():\n",
    "        input_components = create_input_text_components(row)\n",
    "        item_code_for_conditioning = row['중산도면'] # conditioning vector 생성 시 사용할 item 코드\n",
    "        conditioning_vec = create_conditioning_vector(item_code_for_conditioning, machine_df)\n",
    "        target_vec = create_target_vector()\n",
    "        \n",
    "        processed_data_list.append({\n",
    "            \"input_text_components\": input_components, # 실제 input_text 문자열은 다음 단계에서 조립\n",
    "            \"conditioning_vector\": conditioning_vec,\n",
    "            \"target_vector\": target_vec\n",
    "        })\n",
    "    print(f\"\\n총 {len(processed_data_list)}개의 주문 데이터 전처리 완료.\")\n",
    "\n",
    "    # 결과 확인 (첫 2개 데이터 샘플 출력)\n",
    "    if len(processed_data_list) > 0:\n",
    "        print(\"\\n--- 전처리된 데이터 샘플 (첫 2개) ---\")\n",
    "        for i, data_sample in enumerate(processed_data_list[:2]):\n",
    "            print(f\"\\n--- 샘플 {i+1} ---\")\n",
    "            print(f\"Input Text Components: {data_sample['input_text_components']}\")\n",
    "            print(f\"Conditioning Vector: {data_sample['conditioning_vector']}\")\n",
    "            print(f\"Target Vector: {data_sample['target_vector']}\")\n",
    "    else:\n",
    "        print(\"처리된 데이터가 없습니다.\")\n",
    "else:\n",
    "    print(\"order_df가 비어있어 전처리를 수행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b83539-2daa-4d79-bd26-4d75856e4227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 토크나이저 로드 완료.\n",
      "\n",
      "총 127개의 최종 데이터 샘플 생성 완료.\n",
      "\n",
      "--- 토큰화할 텍스트 샘플 ---\n",
      "<DUE>2021-05-13 <ITEM>K04033 <COST>25870 <QTY>318 <URGENT>1\n",
      "\n",
      "--- 토큰화 결과 (Token IDs) ---\n",
      "[27, 35, 8924, 29, 1238, 2481, 12, 2713, 12, 1485, 1279, 2043, 3620, 29, 42, 36676, 2091, 1279, 8220, 2257, 29, 25600, 2154, 1279, 48, 9936, 29, 36042, 1279, 4261, 38, 3525, 29, 16]\n",
      "\n",
      "--- 디코딩된 각 토큰 ---\n",
      "['<', 'D', 'UE', '>', '20', '21', '-', '05', '-', '13', ' <', 'IT', 'EM', '>', 'K', '040', '33', ' <', 'CO', 'ST', '>', '258', '70', ' <', 'Q', 'TY', '>', '318', ' <', 'UR', 'G', 'ENT', '>', '1']\n",
      "\n",
      "--- 토크나이저 어휘 크기 (Vocabulary Size) ---\n",
      "50257\n",
      "\n",
      "--- EOS Token ID ---\n",
      "50256\n",
      "\n",
      "--- PAD Token ID (설정되었다면 EOS와 동일) ---\n",
      "50256\n",
      "\n",
      "--- 최종 데이터 구조 샘플 (첫 번째) ---\n",
      "{\n",
      "  \"input_text\": \"<DUE>2021-05-13 <ITEM>K04033 <COST>25870 <QTY>318 <URGENT>1\",\n",
      "  \"conditioning_vector\": {\n",
      "    \"K04033\": {\n",
      "      \"404\": 12.84,\n",
      "      \"405\": 10.42,\n",
      "      \"407\": 8.69,\n",
      "      \"408\": 9.12,\n",
      "      \"409\": 10.33,\n",
      "      \"410\": 8.99,\n",
      "      \"412\": 12.32,\n",
      "      \"416\": 11.23,\n",
      "      \"422\": 4.0,\n",
      "      \"424\": 9.87,\n",
      "      \"426\": 11.77,\n",
      "      \"433\": 9.54,\n",
      "      \"434\": 13.05,\n",
      "      \"435\": 9.0,\n",
      "      \"436\": 9.0,\n",
      "      \"438\": 4.4,\n",
      "      \"439\": 12.47,\n",
      "      \"440\": 9.4\n",
      "    }\n",
      "  },\n",
      "  \"target_vector\": {\n",
      "    \"output_text\": \"더미 GT 결과: 생산 계획 최적화 완료.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 셀 4: input_text 문자열 생성 및 토큰화 확인\n",
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    "import json # JSON 형태의 출력을 위해 임포트 (선택 사항)\n",
    "\n",
    "# --- GPT-2 토크나이저 로드 ---\n",
    "try:\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "    # GPT-2 토크나이저는 기본적으로 pad_token이 없으므로, eos_token을 pad_token으로 설정 (필요시)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"GPT-2 토크나이저 로드 완료.\")\n",
    "except Exception as e:\n",
    "    print(f\"토크나이저 로드 중 오류 발생: {e}\")\n",
    "    tokenizer = None # 오류 발생 시 None으로 설정\n",
    "\n",
    "# --- input_text 문자열 생성 및 최종 데이터 구조 업데이트 ---\n",
    "final_processed_data_list = []\n",
    "\n",
    "if 'processed_data_list' in globals() and tokenizer: # processed_data_list가 있고 토크나이저가 로드되었는지 확인\n",
    "    for data_sample in processed_data_list:\n",
    "        components = data_sample['input_text_components']\n",
    "        \n",
    "        # input_text_components 딕셔너리의 순서를 유지하며 문자열 생성\n",
    "        # Python 3.7+ 에서는 딕셔너리 삽입 순서가 유지되지만, 명시적으로 순서를 정의하는 것이 더 안전할 수 있음\n",
    "        # 여기서는 딕셔너리 생성 시의 순서(<DUE>, <ITEM>, <COST>, <QTY>, <URGENT>)를 따른다고 가정합니다.\n",
    "        input_text_str = \" \".join([f\"{tag}{value}\" for tag, value in components.items()])\n",
    "        \n",
    "        # 기존 data_sample 딕셔너리를 복사하고 input_text_str 추가, input_text_components는 제거 (선택)\n",
    "        final_sample = {\n",
    "            \"input_text\": input_text_str,\n",
    "            \"conditioning_vector\": data_sample['conditioning_vector'],\n",
    "            \"target_vector\": data_sample['target_vector']\n",
    "        }\n",
    "        final_processed_data_list.append(final_sample)\n",
    "\n",
    "    print(f\"\\n총 {len(final_processed_data_list)}개의 최종 데이터 샘플 생성 완료.\")\n",
    "\n",
    "    # --- 토큰화 결과 확인 (첫 번째 샘플 데이터 대상) ---\n",
    "    if len(final_processed_data_list) > 0:\n",
    "        sample_for_tokenization = final_processed_data_list[0]\n",
    "        text_to_tokenize = sample_for_tokenization['input_text']\n",
    "        \n",
    "        print(f\"\\n--- 토큰화할 텍스트 샘플 ---\")\n",
    "        print(text_to_tokenize)\n",
    "        \n",
    "        # 토큰화 수행\n",
    "        # add_special_tokens=True를 사용하면 문장 시작/끝에 특수 토큰이 추가될 수 있습니다.\n",
    "        # 여기서는 False로 하여 순수 텍스트에 대한 토큰화 결과를 봅니다.\n",
    "        tokenized_output = tokenizer(text_to_tokenize, add_special_tokens=False)\n",
    "        input_ids = tokenized_output['input_ids']\n",
    "        decoded_tokens = [tokenizer.decode([token_id]) for token_id in input_ids]\n",
    "        \n",
    "        print(f\"\\n--- 토큰화 결과 (Token IDs) ---\")\n",
    "        print(input_ids)\n",
    "        \n",
    "        print(f\"\\n--- 디코딩된 각 토큰 ---\")\n",
    "        print(decoded_tokens)\n",
    "        \n",
    "        print(f\"\\n--- 토크나이저 어휘 크기 (Vocabulary Size) ---\")\n",
    "        print(tokenizer.vocab_size)\n",
    "        \n",
    "        print(f\"\\n--- EOS Token ID ---\")\n",
    "        print(tokenizer.eos_token_id)\n",
    "\n",
    "        print(f\"\\n--- PAD Token ID (설정되었다면 EOS와 동일) ---\")\n",
    "        print(tokenizer.pad_token_id)\n",
    "\n",
    "        # 전체 최종 데이터 구조 중 첫 번째 샘플 출력 (JSON 유사 형태)\n",
    "        print(\"\\n--- 최종 데이터 구조 샘플 (첫 번째) ---\")\n",
    "        # json.dumps를 사용하면 딕셔너리를 예쁘게 출력할 수 있습니다.\n",
    "        print(json.dumps(final_processed_data_list[0], indent=2, ensure_ascii=False))\n",
    "        \n",
    "    else:\n",
    "        print(\"토큰화할 데이터가 없습니다 (final_processed_data_list가 비어있음).\")\n",
    "elif not 'processed_data_list' in globals():\n",
    "    print(\"오류: 이전 단계에서 'processed_data_list'가 생성되지 않았습니다.\")\n",
    "elif not tokenizer:\n",
    "    print(\"오류: 토크나이저가 성공적으로 로드되지 않아 토큰화를 진행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64034120-a9a7-4854-b0ce-cf94f672fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "더미 데이터셋 및 데이터로더 생성 완료.\n",
      "Input tensor shape: torch.Size([127, 128])\n",
      "Target tensor shape: torch.Size([127, 128])\n",
      "옵티마이저 설정 완료: <class 'torch.optim.adamw.AdamW'>\n",
      "SEDD 손실 함수 준비 완료.\n",
      "\n",
      "더미 학습 루프 시작...\n",
      "Epoch 1/1, Batch 1/64, Loss: 1014.9778\n",
      "Epoch 1/1, Batch 2/64, Loss: 939.5480\n",
      "Epoch 1/1, Batch 3/64, Loss: 779.2934\n",
      "Epoch 1/1, Batch 4/64, Loss: 764.9310\n",
      "Epoch 1/1, Batch 5/64, Loss: 657.9539\n",
      "Epoch 1/1, Batch 6/64, Loss: 652.9570\n",
      "Epoch 1/1, Batch 7/64, Loss: 503.0675\n",
      "Epoch 1/1, Batch 8/64, Loss: 424.1664\n",
      "Epoch 1/1, Batch 9/64, Loss: 525.7587\n",
      "Epoch 1/1, Batch 10/64, Loss: 645.5414\n",
      "Epoch 1/1, Batch 11/64, Loss: 279.6198\n",
      "Epoch 1/1, Batch 12/64, Loss: 550.5824\n",
      "Epoch 1/1, Batch 13/64, Loss: 399.7316\n",
      "Epoch 1/1, Batch 14/64, Loss: 181.8117\n",
      "Epoch 1/1, Batch 15/64, Loss: 433.5473\n",
      "Epoch 1/1, Batch 16/64, Loss: 289.1126\n",
      "Epoch 1/1, Batch 17/64, Loss: 228.3768\n",
      "Epoch 1/1, Batch 18/64, Loss: 316.6030\n",
      "Epoch 1/1, Batch 19/64, Loss: 293.4620\n",
      "Epoch 1/1, Batch 20/64, Loss: 120.5339\n",
      "Epoch 1/1, Batch 21/64, Loss: 151.9372\n",
      "Epoch 1/1, Batch 22/64, Loss: 336.9583\n",
      "Epoch 1/1, Batch 23/64, Loss: 103.6813\n",
      "Epoch 1/1, Batch 24/64, Loss: 204.8678\n",
      "Epoch 1/1, Batch 25/64, Loss: 236.3867\n",
      "Epoch 1/1, Batch 26/64, Loss: 94.9400\n",
      "Epoch 1/1, Batch 27/64, Loss: 185.4032\n",
      "Epoch 1/1, Batch 28/64, Loss: 97.7650\n",
      "Epoch 1/1, Batch 29/64, Loss: 136.2301\n",
      "Epoch 1/1, Batch 30/64, Loss: 465.5325\n",
      "Epoch 1/1, Batch 31/64, Loss: 158.0507\n",
      "Epoch 1/1, Batch 32/64, Loss: 526.3811\n",
      "Epoch 1/1, Batch 33/64, Loss: 179.2949\n",
      "Epoch 1/1, Batch 34/64, Loss: 226.5253\n",
      "Epoch 1/1, Batch 35/64, Loss: 193.5915\n",
      "Epoch 1/1, Batch 36/64, Loss: 1160.1060\n",
      "Epoch 1/1, Batch 37/64, Loss: 457.0763\n",
      "Epoch 1/1, Batch 38/64, Loss: 111.3132\n",
      "Epoch 1/1, Batch 39/64, Loss: 370.0789\n",
      "Epoch 1/1, Batch 40/64, Loss: 316.0146\n",
      "Epoch 1/1, Batch 41/64, Loss: 243.8983\n",
      "Epoch 1/1, Batch 42/64, Loss: 128.9521\n",
      "Epoch 1/1, Batch 43/64, Loss: 134.5395\n",
      "Epoch 1/1, Batch 44/64, Loss: 181.5119\n",
      "Epoch 1/1, Batch 45/64, Loss: 233.1746\n",
      "Epoch 1/1, Batch 46/64, Loss: 153.8512\n",
      "Epoch 1/1, Batch 47/64, Loss: 172.1442\n",
      "Epoch 1/1, Batch 48/64, Loss: 116.1015\n",
      "Epoch 1/1, Batch 49/64, Loss: 210.7334\n",
      "Epoch 1/1, Batch 50/64, Loss: 187.7570\n",
      "Epoch 1/1, Batch 51/64, Loss: 166.9777\n",
      "Epoch 1/1, Batch 52/64, Loss: 93.5231\n",
      "Epoch 1/1, Batch 53/64, Loss: 307.2737\n",
      "Epoch 1/1, Batch 54/64, Loss: 199.1634\n",
      "Epoch 1/1, Batch 55/64, Loss: 187.2183\n",
      "Epoch 1/1, Batch 56/64, Loss: 145.5430\n",
      "Epoch 1/1, Batch 57/64, Loss: 32.5637\n",
      "Epoch 1/1, Batch 58/64, Loss: 269.8788\n",
      "Epoch 1/1, Batch 59/64, Loss: 120.0706\n",
      "Epoch 1/1, Batch 60/64, Loss: 844.5394\n",
      "Epoch 1/1, Batch 61/64, Loss: 116.8847\n",
      "Epoch 1/1, Batch 62/64, Loss: 156.8481\n",
      "Epoch 1/1, Batch 63/64, Loss: 95.7243\n",
      "Epoch 1/1, Batch 64/64, Loss: 171.0312\n",
      "Epoch 1 완료. 평균 Loss: 318.4940\n",
      "더미 학습 루프 종료.\n"
     ]
    }
   ],
   "source": [
    "# 셀 5: 더미 GT 생성 및 SEDD 모델 파인튜닝 흐름 확인\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# SEDD 레포지토리의 유틸리티 함수 임포트\n",
    "# 경로가 올바르게 설정되어 있어야 합니다. (셀 1에서 처리됨)\n",
    "import losses # losses.py\n",
    "from model import utils as mutils # model/utils.py\n",
    "# import graph_lib # 이미 로드됨 (graph 변수 사용)\n",
    "# import noise_lib # 이미 로드됨 (noise 변수 사용)\n",
    "\n",
    "# --- 하이퍼파라미터 (매우 간소화된 버전) ---\n",
    "dummy_learning_rate = 1e-5\n",
    "dummy_num_epochs = 1 # 실제 학습이 아니므로 1 epoch만\n",
    "dummy_batch_size = 2 # 작은 배치 크기로 테스트\n",
    "max_seq_length = 128 # 모델이 처리할 수 있는 최대 시퀀스 길이 (필요시 SEDD 설정 확인 후 조정)\n",
    "                     # GPT-2는 1024까지 가능하지만, 더미 학습이므로 짧게 설정하여 메모리/시간 절약\n",
    "\n",
    "# --- 데이터셋 및 데이터로더 준비 (더미 학습용) ---\n",
    "# final_processed_data_list를 사용합니다.\n",
    "# 각 \"input_text\"를 토큰화하고, \"target_vector\"의 \"output_text\"도 토큰화합니다.\n",
    "\n",
    "# 1. 입력 텍스트 토큰화\n",
    "input_sequences = []\n",
    "for sample in final_processed_data_list:\n",
    "    # conditioning_vector는 SEDD 모델 직접 입력이 아니므로 여기서는 제외\n",
    "    # 실제로는 모델 아키텍처에 따라 conditioning_vector를 사용하는 방식이 있을 수 있음\n",
    "    tokenized_input = tokenizer(sample['input_text'], \n",
    "                                padding='max_length',      # 최대 길이에 맞춰 패딩\n",
    "                                truncation=True,           # 최대 길이 초과시 자르기\n",
    "                                max_length=max_seq_length, \n",
    "                                return_tensors=\"pt\")       # PyTorch 텐서로 반환\n",
    "    input_sequences.append(tokenized_input['input_ids'].squeeze(0)) # 배치 차원 제거\n",
    "\n",
    "# 2. 타겟 텍스트(더미 GT) 토큰화\n",
    "# 모든 샘플에 대해 동일한 더미 타겟을 사용하거나, 입력과 관련된 간단한 타겟을 만들 수 있습니다.\n",
    "# 여기서는 \"input_text\"의 처음 몇 토큰을 반복하는 것으로 가정 (매우 단순한 예시)\n",
    "# 또는 고정된 더미 텍스트 사용\n",
    "dummy_gt_text = \"더미 GT 결과입니다.\" # 모든 샘플에 동일한 GT 사용\n",
    "tokenized_dummy_gt = tokenizer(dummy_gt_text,\n",
    "                               padding='max_length',\n",
    "                               truncation=True,\n",
    "                               max_length=max_seq_length,\n",
    "                               return_tensors=\"pt\")['input_ids'].squeeze(0)\n",
    "\n",
    "target_sequences = [tokenized_dummy_gt.clone() for _ in range(len(input_sequences))]\n",
    "\n",
    "\n",
    "# PyTorch Dataset 및 DataLoader 생성\n",
    "if input_sequences and target_sequences:\n",
    "    input_tensor = torch.stack(input_sequences)\n",
    "    target_tensor = torch.stack(target_sequences)\n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=dummy_batch_size, shuffle=True)\n",
    "    print(f\"\\n더미 데이터셋 및 데이터로더 생성 완료.\")\n",
    "    print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "    print(f\"Target tensor shape: {target_tensor.shape}\")\n",
    "else:\n",
    "    print(\"\\n오류: 입력 또는 타겟 시퀀스가 비어있어 데이터로더를 생성할 수 없습니다.\")\n",
    "    dataloader = None\n",
    "\n",
    "# --- 모델, 옵티마이저, 손실 함수 등 설정 ---\n",
    "if dataloader:\n",
    "    # 모델을 학습 모드로 설정\n",
    "    score_model.train()\n",
    "\n",
    "    # 옵티마이저 설정 (SEDD 레포의 losses.get_optimizer 사용 가능)\n",
    "    # 간소화를 위해 AdamW 직접 사용\n",
    "    # 실제 SEDD 학습에는 config 객체가 필요하므로, 여기서는 단순화합니다.\n",
    "    optimizer = optim.AdamW(score_model.parameters(), lr=dummy_learning_rate)\n",
    "    print(f\"옵티마이저 설정 완료: {type(optimizer)}\")\n",
    "\n",
    "    # 손실 함수 설정 (SEDD 레포의 losses.get_loss_fn 사용)\n",
    "    # get_loss_fn에는 noise, graph, train 인자가 필요합니다.\n",
    "    # sampling_eps 등 추가 파라미터는 기본값 사용 가능\n",
    "    # 이 loss_fn은 (model, batch, cond, t, perturbed_batch) 등을 인자로 받음\n",
    "    # 현재 우리는 조건(cond), 시간(t), perturbed_batch를 명시적으로 제어하지 않으므로,\n",
    "    # SEDD의 학습 방식에 맞게 batch (토큰 ID 시퀀스)만 전달하는 형태로\n",
    "    # 실제 loss 계산이 이루어지는지 확인해야 합니다.\n",
    "    # SEDD의 loss_fn은 내부적으로 noise 샘플링, 교란 등을 수행합니다.\n",
    "    \n",
    "    # SEDD의 loss_fn 시그니처에 맞게 더미 학습 루프 조정 필요\n",
    "    # loss_fn_sedd = losses.get_loss_fn(noise.to(device), graph, train=True)\n",
    "    # 위 noise.to(device)는 이미 device에 있으므로 중복일 수 있습니다.\n",
    "    # load_model에서 반환된 noise, graph 객체를 사용합니다.\n",
    "    loss_fn_sedd = losses.get_loss_fn(noise, graph, train=True)\n",
    "    print(f\"SEDD 손실 함수 준비 완료.\")\n",
    "\n",
    "    # --- 더미 학습 루프 ---\n",
    "    print(\"\\n더미 학습 루프 시작...\")\n",
    "    for epoch in range(dummy_num_epochs):\n",
    "        total_loss_epoch = 0\n",
    "        for batch_idx, (input_batch, target_batch) in enumerate(dataloader):\n",
    "            input_batch = input_batch.to(device)\n",
    "            # target_batch는 SEDD의 score_entropy loss 계산 시 x0 (원본 데이터)로 사용될 수 있습니다.\n",
    "            # SEDD의 loss_fn은 (model, batch_x0, cond=None, t=None, perturbed_batch=None) 형태로 호출,\n",
    "            # 여기서 batch_x0가 원본 데이터를 의미합니다.\n",
    "            # 그리고 이 batch_x0로부터 내부적으로 perturbed_batch를 생성합니다.\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # SEDD의 get_loss_fn으로 생성된 loss_fn은 (model, batch)를 받습니다.\n",
    "            # 이 batch가 x_0 (원본 깨끗한 데이터)에 해당합니다.\n",
    "            # loss_fn 내부에서 t 샘플링, 노이즈 추가, 모델 호출, score_entropy 계산 등이 이루어집니다.\n",
    "            try:\n",
    "                # score_model은 DDP로 감싸져 있을 수 있으나, loss_fn_sedd 내부에서 처리될 것으로 기대\n",
    "                loss = loss_fn_sedd(score_model, input_batch) \n",
    "                \n",
    "                # loss가 텐서 리스트 등으로 반환될 경우 .mean() 등이 필요할 수 있음\n",
    "                if isinstance(loss, list) or (isinstance(loss, torch.Tensor) and loss.ndim > 0 and loss.numel() > 1) :\n",
    "                    loss = loss.mean() \n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss_epoch += loss.item()\n",
    "                \n",
    "                if batch_idx % 1 == 0: # 모든 배치마다 로그 (더미이므로)\n",
    "                    print(f\"Epoch {epoch+1}/{dummy_num_epochs}, Batch {batch_idx+1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "                \n",
    "                # 아주 기본적인 확인: gradient가 흐르는지 (옵션)\n",
    "                # if batch_idx == 0:\n",
    "                #     for name, param in score_model.named_parameters():\n",
    "                #         if param.grad is not None:\n",
    "                #             print(f\"Gradient found for: {name}, grad norm: {param.grad.norm().item()}\")\n",
    "                #         else:\n",
    "                #             print(f\"No gradient for: {name}\")\n",
    "                #         if 'vocab_embed' in name: # 너무 많으니 일부만\n",
    "                #             break\n",
    "                            \n",
    "            except Exception as e_train:\n",
    "                print(f\"학습 중 오류 발생 (Epoch {epoch+1}, Batch {batch_idx+1}): {e_train}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                break # 오류 발생 시 해당 에폭 중단\n",
    "        \n",
    "        avg_loss_epoch = total_loss_epoch / len(dataloader) if len(dataloader) > 0 else 0\n",
    "        print(f\"Epoch {epoch+1} 완료. 평균 Loss: {avg_loss_epoch:.4f}\")\n",
    "        if 'e_train' in locals() and e_train: break # 학습 중 오류 시 전체 루프 중단\n",
    "\n",
    "    print(\"더미 학습 루프 종료.\")\n",
    "else:\n",
    "    print(\"데이터로더가 생성되지 않아 학습 루프를 실행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d63b1f-5e14-4f35-8014-fd6b0678987f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 다양한 문자열 요소 및 특수 기호 토큰화 테스트 ---\n",
      "\n",
      "--- 테스트: 날짜 (YYYY-MM-DD) ---\n",
      "원본 텍스트: '2021-08-21'\n",
      "Token IDs: [1238, 2481, 12, 2919, 12, 2481]\n",
      "디코딩된 각 토큰: ['20', '21', '-', '08', '-', '21']\n",
      "생성된 토큰 수: 6\n",
      "\n",
      "--- 테스트: 품목 코드 (알파벳+숫자) ---\n",
      "원본 텍스트: 'K0123'\n",
      "Token IDs: [42, 486, 1954]\n",
      "디코딩된 각 토큰: ['K', '01', '23']\n",
      "생성된 토큰 수: 3\n",
      "\n",
      "--- 테스트: 품목 코드 (숫자형 문자열) ---\n",
      "원본 텍스트: '057387'\n",
      "Token IDs: [43526, 32220]\n",
      "디코딩된 각 토큰: ['057', '387']\n",
      "생성된 토큰 수: 2\n",
      "\n",
      "--- 테스트: 품목 코드 (이전 예시) ---\n",
      "원본 텍스트: 'K04033'\n",
      "Token IDs: [42, 36676, 2091]\n",
      "디코딩된 각 토큰: ['K', '040', '33']\n",
      "생성된 토큰 수: 3\n",
      "\n",
      "--- 테스트: 단가 (큰 숫자) ---\n",
      "원본 텍스트: '25870'\n",
      "Token IDs: [25600, 2154]\n",
      "디코딩된 각 토큰: ['258', '70']\n",
      "생성된 토큰 수: 2\n",
      "\n",
      "--- 테스트: 수량 (작은 숫자) ---\n",
      "원본 텍스트: '66'\n",
      "Token IDs: [2791]\n",
      "디코딩된 각 토큰: ['66']\n",
      "생성된 토큰 수: 1\n",
      "\n",
      "--- 테스트: 태그 포함 날짜 ---\n",
      "원본 텍스트: '<DUE>2021-08-21'\n",
      "Token IDs: [27, 35, 8924, 29, 1238, 2481, 12, 2919, 12, 2481]\n",
      "디코딩된 각 토큰: ['<', 'D', 'UE', '>', '20', '21', '-', '08', '-', '21']\n",
      "생성된 토큰 수: 10\n",
      "\n",
      "--- 테스트: 태그 포함 품목코드 ---\n",
      "원본 텍스트: '<ITEM>K0123'\n",
      "Token IDs: [27, 2043, 3620, 29, 42, 486, 1954]\n",
      "디코딩된 각 토큰: ['<', 'IT', 'EM', '>', 'K', '01', '23']\n",
      "생성된 토큰 수: 7\n",
      "\n",
      "--- 테스트: 불릿 포인트 ---\n",
      "원본 텍스트: '• IMPORT'\n",
      "Token IDs: [3581, 30023, 9863]\n",
      "디코딩된 각 토큰: ['•', ' IMP', 'ORT']\n",
      "생성된 토큰 수: 3\n",
      "\n",
      "--- 테스트: 슬래시 ---\n",
      "원본 텍스트: 'A/B 테스트'\n",
      "Token IDs: [32, 14, 33, 220, 169, 227, 234, 168, 232, 97, 169, 232, 116]\n",
      "디코딩된 각 토큰: ['A', '/', 'B', ' ', '�', '�', '�', '�', '�', '�', '�', '�', '�']\n",
      "생성된 토큰 수: 13\n",
      "\n",
      "--- 테스트: 줄바꿈 (단일) ---\n",
      "원본 텍스트: 'FIRST\n",
      "SECOND'\n",
      "Token IDs: [39776, 2257, 198, 23683, 18672]\n",
      "디코딩된 각 토큰: ['FIR', 'ST', '\\n', 'SEC', 'OND']\n",
      "생성된 토큰 수: 5\n",
      "\n",
      "--- 테스트: 줄바꿈 (연속) ---\n",
      "원본 텍스트: 'FIRST\n",
      "\n",
      "SECOND'\n",
      "Token IDs: [39776, 2257, 198, 198, 23683, 18672]\n",
      "디코딩된 각 토큰: ['FIR', 'ST', '\\n', '\\n', 'SEC', 'OND']\n",
      "생성된 토큰 수: 6\n",
      "\n",
      "--- 테스트: 콤마 ---\n",
      "원본 텍스트: 'APPLE,BANANA,STRAW'\n",
      "Token IDs: [2969, 16437, 11, 33, 1565, 31574, 11, 2257, 20530]\n",
      "디코딩된 각 토큰: ['AP', 'PLE', ',', 'B', 'AN', 'ANA', ',', 'ST', 'RAW']\n",
      "생성된 토큰 수: 9\n",
      "\n",
      "--- 테스트: 파이프 ---\n",
      "원본 텍스트: 'OPT1|OPT2|OPT3'\n",
      "Token IDs: [3185, 51, 16, 91, 3185, 51, 17, 91, 3185, 51, 18]\n",
      "디코딩된 각 토큰: ['OP', 'T', '1', '|', 'OP', 'T', '2', '|', 'OP', 'T', '3']\n",
      "생성된 토큰 수: 11\n",
      "\n",
      "--- 테스트: 전체 input_text 예시 (다양한 값 포함) ---\n",
      "원본 텍스트: '<DUE>2024-12-31 <ITEM>NEWITEM001 <COST>123456 <QTY>789 <URGENT>2'\n",
      "Token IDs: [27, 35, 8924, 29, 1238, 1731, 12, 1065, 12, 3132, 1279, 2043, 3620, 29, 13965, 2043, 3620, 8298, 1279, 8220, 2257, 29, 10163, 29228, 1279, 48, 9936, 29, 40401, 1279, 4261, 38, 3525, 29, 17]\n",
      "디코딩된 각 토큰: ['<', 'D', 'UE', '>', '20', '24', '-', '12', '-', '31', ' <', 'IT', 'EM', '>', 'NEW', 'IT', 'EM', '001', ' <', 'CO', 'ST', '>', '123', '456', ' <', 'Q', 'TY', '>', '789', ' <', 'UR', 'G', 'ENT', '>', '2']\n",
      "생성된 토큰 수: 35\n"
     ]
    }
   ],
   "source": [
    "# 셀 6: 다양한 데이터 요소 및 특수 기호 토큰화 상세 확인\n",
    "\n",
    "if 'tokenizer' in globals() and tokenizer:\n",
    "    test_strings = {\n",
    "        \"날짜 (YYYY-MM-DD)\": \"2021-08-21\",\n",
    "        \"품목 코드 (알파벳+숫자)\": \"K0123\",\n",
    "        \"품목 코드 (숫자형 문자열)\": \"057387\",\n",
    "        \"품목 코드 (이전 예시)\": \"K04033\",\n",
    "        \"단가 (큰 숫자)\": \"25870\",\n",
    "        \"수량 (작은 숫자)\": \"66\",\n",
    "        \"태그 포함 날짜\": \"<DUE>2021-08-21\",\n",
    "        \"태그 포함 품목코드\": \"<ITEM>K0123\",\n",
    "        \"불릿 포인트\": \"• IMPORT\",\n",
    "        \"슬래시\": \"A/B 테스트\",\n",
    "        \"줄바꿈 (단일)\": \"FIRST\\nSECOND\",\n",
    "        \"줄바꿈 (연속)\": \"FIRST\\n\\nSECOND\",\n",
    "        \"콤마\": \"APPLE,BANANA,STRAW\",\n",
    "        \"파이프\": \"OPT1|OPT2|OPT3\",\n",
    "        \"전체 input_text 예시 (다양한 값 포함)\": \"<DUE>2024-12-31 <ITEM>NEWITEM001 <COST>123456 <QTY>789 <URGENT>2\"\n",
    "    }\n",
    "\n",
    "    print(\"--- 다양한 문자열 요소 및 특수 기호 토큰화 테스트 ---\")\n",
    "    for description, text in test_strings.items():\n",
    "        print(f\"\\n--- 테스트: {description} ---\")\n",
    "        print(f\"원본 텍스트: '{text}'\")\n",
    "        \n",
    "        # 토큰화 (add_special_tokens=False로 하여 순수 텍스트에 대한 결과 확인)\n",
    "        tokenized_output = tokenizer(text, add_special_tokens=False)\n",
    "        input_ids = tokenized_output['input_ids']\n",
    "        decoded_tokens = [tokenizer.decode([token_id]) for token_id in input_ids]\n",
    "        \n",
    "        print(f\"Token IDs: {input_ids}\")\n",
    "        print(f\"디코딩된 각 토큰: {decoded_tokens}\")\n",
    "        print(f\"생성된 토큰 수: {len(input_ids)}\")\n",
    "\n",
    "else:\n",
    "    print(\"오류: 토크나이저가 로드되지 않았습니다. 이전 셀을 먼저 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89060e03-dc8a-453e-89c4-e86ef69910c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using predictor: euler, steps: 128, denoise: True (from model config if available)\n",
      "샘플링 함수 준비 완료.\n",
      "\n",
      "2개의 텍스트 샘플을 생성합니다 (길이: 256, 스텝: 128)...\n",
      "텍스트 생성 완료.\n",
      "\n",
      "--- 생성된 텍스트 샘플 (Total: 2) ---\n",
      "\n",
      "--- 샘플 1 ---\n",
      " <27> <28> <46> <46> <46> <47> <48> <49> <49> <75> <75> <75> <47> <68> <67> <72> <111> <158> <200> <141> <61> <62> <63> <65> <36 <25> <25> <25> <25> <75 <60> <65> <63> <60> <64> <64> <69> <68> <23> <67> <72> <16> <2048> <2048> <25> <37> <16> <64> <63> <63> <65> <2400> <34> <34> <21> <2025> <50> <73> <48> <72> <53> <54> <36> <58> <57> <59> <64> <65> <62> <1850> <36> <21> <25 <25> <36> <54> <44> <48> <18> <1850 < <47> <40> <53> <100> <\n",
      "--------------------------------------------------\n",
      "\n",
      "--- 샘플 2 ---\n",
      " Rover showcased 5.5-speed P vehicles in 2011 as the leading collaborative effort in robotics engineering.\n",
      "\n",
      "Unprecedented access to automation and capabilities networks As these highperformance supercars, and fast- and in-line traffic cars, have embraced autonomous automation globally, today’s automotive operators, with no laying off capacity to deliver technical solutions to luxury cars and service providers, are positioned forever to leverage advanced network solutions and support innovative solutions with flexibility in addition to local sharing, by reducing the cost of per-layer network management and to enable increasing access to distributed autonomous systems.\n",
      "\n",
      "Once it was once unthinkable to provide individual mobility by driver, tech solutions that support these advanced platforms are simply not there anymore\n",
      "\n",
      "Originally Released Toyota Escape® R1000 sportcar Ford Escape OG Volkswagen Purify Model Rhode Island Cuocino Falcon, 2015 Ohio Nissan Insight Mini Nissan Leaf Volkswagen Santa Clara VAX® BMW San Francisco Tesla North Carolina Cayenne Hyundai SHIE IMAX® Honda Odyssey Ducati Car and Driver Connection Microsoft Car and Driver Claudo Master Car and Driver Robotics Deliveries Honda Accord Lynx Hewlett-Packard Home WiFi Fiat Chrysler Land Cruiser TVS Todotta Pontiac Showroom Hyundai Focus Si General Electric DRARGY Toyota Show Project Saole Cus\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 셀 7: 사전학습된 SEDD 모델로 조건 없는 텍스트 샘플링 (Inference)\n",
    "\n",
    "import torch\n",
    "import sampling # Score-Entropy-Discrete-Diffusion 레포 내의 sampling.py\n",
    "\n",
    "# --- 샘플링 관련 하이퍼파라미터 ---\n",
    "num_samples_to_generate = 2  # 생성할 샘플 개수\n",
    "sampling_steps = 128         # 샘플링 스텝 수 (SEDD config의 기본값 또는 조절 가능)\n",
    "                             # run_sample.py 에서는 1024를 사용하기도 하지만, 테스트를 위해 줄여도 됩니다.\n",
    "                             # 스텝 수가 적으면 품질이 낮아지고, 많으면 오래 걸립니다.\n",
    "sequence_length = 256        # 생성할 텍스트의 길이 (토큰 기준)\n",
    "                             # 모델은 1024까지 학습되었지만, 테스트를 위해 짧게 설정.\n",
    "sampling_eps = 1e-5          # 샘플링 시 사용되는 작은 epsilon 값 (SEDD 기본값)\n",
    "\n",
    "# --- 샘플링 함수 가져오기 ---\n",
    "# get_pc_sampler(graph, noise, batch_dims, predictor, steps, denoise=True, eps=1e-5, device=torch.device('cpu'), proj_fun=lambda x: x)\n",
    "# batch_dims는 (생성할 샘플 수, 시퀀스 길이)\n",
    "# predictor는 SEDD config에서 'euler' 등을 사용할 수 있음 (config.sampling.predictor)\n",
    "# louaaron/sedd-small의 config.json을 보면 \"sampling\": {\"predictor\": \"euler\", \"steps\": 128, \"noise_removal\": true} 로 되어 있습니다.\n",
    "# 우리 모델 로드 시 config 정보가 score_model.config에 저장되어 있을 수 있습니다.\n",
    "\n",
    "try:\n",
    "    if hasattr(score_model, 'config') and score_model.config:\n",
    "        predictor_type = score_model.config.sampling.predictor\n",
    "        # sampling_steps = score_model.config.sampling.steps # config의 스텝 수를 사용할 수도 있음\n",
    "        denoise_sampling = score_model.config.sampling.noise_removal\n",
    "        print(f\"Using predictor: {predictor_type}, steps: {sampling_steps}, denoise: {denoise_sampling} (from model config if available)\")\n",
    "    else:\n",
    "        # score_model.config가 없을 경우 기본값 사용\n",
    "        predictor_type = 'euler' # SEDD의 일반적인 기본값\n",
    "        denoise_sampling = True\n",
    "        print(f\"Warning: Model config not found or empty. Using default predictor: {predictor_type}, steps: {sampling_steps}, denoise: {denoise_sampling}\")\n",
    "\n",
    "    # 조건 없는 샘플링이므로 proj_fun은 기본값 (lambda x: x) 사용\n",
    "    # get_pc_sampler의 batch_dims 인자는 (batch_size, sequence_length)\n",
    "    # 여기서 batch_size는 한 번에 생성할 샘플의 수\n",
    "    unconditional_sampling_fn = sampling.get_pc_sampler(\n",
    "        graph=graph,\n",
    "        noise=noise,\n",
    "        batch_dims=(num_samples_to_generate, sequence_length), # (생성할 샘플 수, 각 샘플의 길이)\n",
    "        predictor=predictor_type,\n",
    "        steps=sampling_steps,\n",
    "        denoise=denoise_sampling,\n",
    "        eps=sampling_eps,\n",
    "        device=device\n",
    "    )\n",
    "    print(\"샘플링 함수 준비 완료.\")\n",
    "\n",
    "    # --- 텍스트 생성 수행 ---\n",
    "    print(f\"\\n{num_samples_to_generate}개의 텍스트 샘플을 생성합니다 (길이: {sequence_length}, 스텝: {sampling_steps})...\")\n",
    "    \n",
    "    # 모델을 평가 모드로 설정\n",
    "    score_model.eval() \n",
    "    \n",
    "    with torch.no_grad(): # 그래디언트 계산 비활성화\n",
    "        generated_samples_ids = unconditional_sampling_fn(score_model)\n",
    "    \n",
    "    print(\"텍스트 생성 완료.\")\n",
    "\n",
    "    # --- 생성된 토큰 ID를 텍스트로 디코딩 ---\n",
    "    if generated_samples_ids is not None:\n",
    "        print(f\"\\n--- 생성된 텍스트 샘플 (Total: {generated_samples_ids.shape[0]}) ---\")\n",
    "        generated_texts = tokenizer.batch_decode(generated_samples_ids, skip_special_tokens=True)\n",
    "        \n",
    "        for i, text in enumerate(generated_texts):\n",
    "            print(f\"\\n--- 샘플 {i+1} ---\")\n",
    "            print(text)\n",
    "            print(\"-\" * 50) # 구분선\n",
    "    else:\n",
    "        print(\"생성된 샘플이 없습니다.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"샘플링 중 오류 발생: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8799de81-d546-462c-968d-d0885d9f2627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "조건부 입력 텍스트: '<DUE>2021-05-13 <ITEM>K04033 <COST>25870 <QTY>318 <URGENT>1'\n",
      "조건부 입력 토큰 수: 34\n",
      "조건부 샘플링 함수 준비 완료.\n",
      "\n",
      "조건부 텍스트 샘플을 생성합니다 (조건 길이: 34, 추가 생성 길이: 128, 총 길이: 162)...\n",
      "조건부 텍스트 생성 완료.\n",
      "\n",
      "--- 생성된 조건부 텍스트 샘플 ---\n",
      "\n",
      "--- 샘플 1 ---\n",
      "<DUE>2021-05-13 <ITEM>K04033 <COST>25870 <QTY>318 <URGENT>1125 <MARINE>128 <MIND>56 <MIND>396 <MIND>5ABC>FAIRID=1 <CHORRIO>2\n",
      "\n",
      "SNIPESS <$6.29 <TENNARK $2.51 <JIVE 50450 $2.1254 <ANOMY $2.86 KOMG 8 <OOMY $2.81 <HAT 3$1.15 <HATTAR>4 ALTEN <$59.51>IN 84469 323434 <HIMRIGHT> 95443 GEUGEDIED 4\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 셀 8: 조건부 텍스트 생성 (Prefix Conditional Generation)\n",
    "\n",
    "import torch\n",
    "import sampling # Score-Entropy-Discrete-Diffusion 레포 내의 sampling.py\n",
    "# from transformers import GPT2TokenizerFast # 이미 로드되어 있다고 가정 (tokenizer 변수)\n",
    "\n",
    "# --- 추론 설정 ---\n",
    "# 예시 input_text (실제 파인튜닝 시 사용했던 형식과 일치해야 함)\n",
    "# 현재 final_processed_data_list의 첫 번째 샘플의 input_text를 사용한다고 가정\n",
    "if 'final_processed_data_list' in globals() and len(final_processed_data_list) > 0:\n",
    "    conditional_input_text = final_processed_data_list[0]['input_text']\n",
    "else:\n",
    "    # fallback 예시 (실제 데이터로 교체 필요)\n",
    "    conditional_input_text = \"<DUE>2021-05-13 <ITEM>K04033 <COST>25870 <QTY>318 <URGENT>1\"\n",
    "    print(f\"Warning: final_processed_data_list 를 찾을 수 없어 기본 conditional_input_text를 사용합니다: {conditional_input_text}\")\n",
    "\n",
    "num_conditional_samples = 1 # 생성할 조건부 샘플 개수\n",
    "generated_text_max_length = 128 # 조건부 입력 뒤에 생성될 텍스트의 최대 길이 (토큰 기준)\n",
    "                               # 전체 시퀀스 길이는 conditional_input_tokens + generated_text_max_length 가 됩니다.\n",
    "                               # 모델의 최대 시퀀스 길이 (예: 1024)를 넘지 않도록 주의해야 합니다.\n",
    "sampling_steps_conditional = 128 # 조건부 샘플링 스텝 수\n",
    "sampling_eps_conditional = 1e-5\n",
    "\n",
    "# --- 조건부 입력을 토큰 ID로 변환 ---\n",
    "if 'tokenizer' in globals() and tokenizer:\n",
    "    conditional_input_tokens = tokenizer(conditional_input_text, add_special_tokens=False)['input_ids']\n",
    "    num_conditional_tokens = len(conditional_input_tokens)\n",
    "    print(f\"조건부 입력 텍스트: '{conditional_input_text}'\")\n",
    "    print(f\"조건부 입력 토큰 수: {num_conditional_tokens}\")\n",
    "\n",
    "    # 전체 생성될 시퀀스 길이 (조건부 입력 + 새로 생성될 부분)\n",
    "    total_sequence_length = num_conditional_tokens + generated_text_max_length\n",
    "    # 모델의 최대 컨텍스트 길이를 초과하지 않도록 조정 (예: SEDD small은 1024)\n",
    "    # 실제 모델의 config.model.length 값을 확인해야 합니다.\n",
    "    # louaaron/sedd-small 의 config.json 에는 model.length=1024 로 되어 있음\n",
    "    model_max_len = score_model.config.model.length if hasattr(score_model, 'config') and score_model.config else 1024\n",
    "    if total_sequence_length > model_max_len:\n",
    "        print(f\"Warning: 요청된 총 시퀀스 길이({total_sequence_length})가 모델 최대 길이({model_max_len})를 초과합니다. 생성 길이를 줄입니다.\")\n",
    "        generated_text_max_length = model_max_len - num_conditional_tokens\n",
    "        total_sequence_length = model_max_len\n",
    "        if generated_text_max_length <= 0:\n",
    "            print(\"오류: 조건부 입력이 너무 길어 추가 생성이 불가능합니다.\")\n",
    "            # 이 경우 샘플링 중단\n",
    "            generated_texts_conditional = [\"오류: 조건부 입력이 너무 김\"]\n",
    "\n",
    "\n",
    "    # --- 조건부 샘플링을 위한 proj_fun 정의 ---\n",
    "    # 이 함수는 샘플링 각 스텝에서 conditional_input_tokens 부분을 고정시킵니다.\n",
    "    def conditional_proj_fun(x_t_sampled):\n",
    "        # x_t_sampled의 모양: (num_conditional_samples, total_sequence_length)\n",
    "        # conditional_input_tokens_tensor의 모양: (num_conditional_tokens)\n",
    "        if 'conditional_input_tokens_tensor_expanded' not in conditional_proj_fun.__dict__:\n",
    "             # 함수 내부에 텐서를 저장하여 반복 생성을 피함 (클로저와 유사)\n",
    "            conditional_proj_fun.conditional_input_tokens_tensor_expanded = \\\n",
    "                torch.tensor(conditional_input_tokens, device=device).unsqueeze(0).expand(num_conditional_samples, -1)\n",
    "\n",
    "        x_t_sampled[:, :num_conditional_tokens] = conditional_proj_fun.conditional_input_tokens_tensor_expanded\n",
    "        return x_t_sampled\n",
    "        \n",
    "    # --- 조건부 샘플링 함수 가져오기 ---\n",
    "    try:\n",
    "        if hasattr(score_model, 'config') and score_model.config:\n",
    "            predictor_type_cond = score_model.config.sampling.predictor\n",
    "            denoise_sampling_cond = score_model.config.sampling.noise_removal\n",
    "        else:\n",
    "            predictor_type_cond = 'euler'\n",
    "            denoise_sampling_cond = True\n",
    "        \n",
    "        conditional_sampling_fn = sampling.get_pc_sampler(\n",
    "            graph=graph,\n",
    "            noise=noise,\n",
    "            batch_dims=(num_conditional_samples, total_sequence_length), # 전체 시퀀스 길이에 대한 샘플링\n",
    "            predictor=predictor_type_cond,\n",
    "            steps=sampling_steps_conditional,\n",
    "            denoise=denoise_sampling_cond,\n",
    "            eps=sampling_eps_conditional,\n",
    "            device=device,\n",
    "            proj_fun=conditional_proj_fun # 조건 고정 함수 전달\n",
    "        )\n",
    "        print(\"조건부 샘플링 함수 준비 완료.\")\n",
    "\n",
    "        # --- 조건부 텍스트 생성 수행 ---\n",
    "        print(f\"\\n조건부 텍스트 샘플을 생성합니다 (조건 길이: {num_conditional_tokens}, 추가 생성 길이: {generated_text_max_length}, 총 길이: {total_sequence_length})...\")\n",
    "        \n",
    "        score_model.eval()\n",
    "        with torch.no_grad():\n",
    "            # 초기 노이즈는 전체 시퀀스 길이에 대해 생성됨\n",
    "            # conditional_proj_fun에 의해 앞부분이 고정되면서 샘플링 진행\n",
    "            generated_conditional_ids = conditional_sampling_fn(score_model) \n",
    "            # 샘플링 후에도 한 번 더 proj_fun을 적용하여 조건부 고정 보장 (선택적이지만 안전)\n",
    "            generated_conditional_ids = conditional_proj_fun(generated_conditional_ids)\n",
    "\n",
    "        print(\"조건부 텍스트 생성 완료.\")\n",
    "\n",
    "        # --- 생성된 텍스트 디코딩 ---\n",
    "        print(f\"\\n--- 생성된 조건부 텍스트 샘플 ---\")\n",
    "        # 전체 시퀀스를 디코딩합니다.\n",
    "        generated_texts_conditional = tokenizer.batch_decode(generated_conditional_ids, skip_special_tokens=True)\n",
    "        \n",
    "        for i, text in enumerate(generated_texts_conditional):\n",
    "            print(f\"\\n--- 샘플 {i+1} ---\")\n",
    "            print(text)\n",
    "            # 생성된 부분만 따로 보고 싶다면:\n",
    "            # generated_part = tokenizer.decode(generated_conditional_ids[i, num_conditional_tokens:], skip_special_tokens=True)\n",
    "            # print(f\"생성된 부분: {generated_part}\")\n",
    "            print(\"-\" * 70)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"조건부 샘플링 중 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"오류: 토크나이저가 로드되지 않았거나 final_processed_data_list가 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff7961ae-50a1-4249-b718-9a2d7dd75d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix: 'The weather today is' (4 토큰)\n",
      "Suffix: 'so I decided to stay home.' (7 토큰)\n",
      "Infilling 샘플링 함수 준비 완료.\n",
      "\n",
      "Infilling 텍스트 샘플을 생성합니다 (총 길이: 64)...\n",
      "Infilling 텍스트 생성 완료.\n",
      "\n",
      "--- 생성된 Infilling 텍스트 샘플 ---\n",
      "\n",
      "--- 샘플 1 ---\n",
      "The weather today is a short sunny, sunny day with a good economy. Dont’t those days turn out a little more positive with the weather but because of the good economy—a little “Saturday and Monday’ Day” and because of the good economy—so I decided to stay home.\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 셀 9: Infilling (내용 채우기) 방식의 조건부 추론\n",
    "\n",
    "import torch\n",
    "import sampling # Score-Entropy-Discrete-Diffusion 레포 내의 sampling.py\n",
    "# from transformers import GPT2TokenizerFast # 이미 로드되어 있다고 가정 (tokenizer 변수)\n",
    "\n",
    "# --- Infilling 설정 ---\n",
    "prefix_text = \"The weather today is\"\n",
    "suffix_text = \"so I decided to stay home.\"\n",
    "# 예시: \"The weather today is [모델이 생성한 내용] so I decided to stay home.\"\n",
    "\n",
    "num_infilling_samples = 1   # 생성할 Infilling 샘플 개수\n",
    "infilling_total_sequence_length = 64 # Prefix + 생성부분 + Suffix 를 포함한 전체 시퀀스 길이\n",
    "                                     # 이 길이가 너무 짧으면 prefix/suffix가 겹치거나 생성 공간이 없을 수 있음\n",
    "                                     # 모델의 최대 시퀀스 길이 (예: 1024)를 넘지 않도록 주의\n",
    "sampling_steps_infilling = 128\n",
    "sampling_eps_infilling = 1e-5\n",
    "\n",
    "# --- Prefix 및 Suffix 토큰화 ---\n",
    "if 'tokenizer' in globals() and tokenizer:\n",
    "    prefix_ids = tokenizer(prefix_text, add_special_tokens=False)['input_ids']\n",
    "    suffix_ids = tokenizer(suffix_text, add_special_tokens=False)['input_ids']\n",
    "    \n",
    "    num_prefix_tokens = len(prefix_ids)\n",
    "    num_suffix_tokens = len(suffix_ids)\n",
    "\n",
    "    print(f\"Prefix: '{prefix_text}' ({num_prefix_tokens} 토큰)\")\n",
    "    print(f\"Suffix: '{suffix_text}' ({num_suffix_tokens} 토큰)\")\n",
    "\n",
    "    if num_prefix_tokens + num_suffix_tokens >= infilling_total_sequence_length:\n",
    "        print(\"오류: Prefix와 Suffix의 길이 합이 전체 시퀀스 길이보다 크거나 같습니다. Infilling 공간이 없습니다.\")\n",
    "        # 이 경우 샘플링 중단 또는 파라미터 조정 필요\n",
    "        generated_texts_infilling = [\"오류: Infilling 공간 부족\"]\n",
    "    else:\n",
    "        # --- Infilling을 위한 proj_fun 정의 ---\n",
    "        # 전체 시퀀스에서 prefix와 suffix 위치의 토큰들을 고정\n",
    "        \n",
    "        # 고정될 토큰 ID 텐서 준비 (배치 차원 추가)\n",
    "        # prefix 부분\n",
    "        prefix_ids_tensor = torch.tensor(prefix_ids, device=device).unsqueeze(0).expand(num_infilling_samples, -1)\n",
    "        # suffix 부분\n",
    "        suffix_ids_tensor = torch.tensor(suffix_ids, device=device).unsqueeze(0).expand(num_infilling_samples, -1)\n",
    "\n",
    "        def infilling_proj_fun(x_t_sampled):\n",
    "            # x_t_sampled의 모양: (num_infilling_samples, infilling_total_sequence_length)\n",
    "            \n",
    "            # Prefix 고정\n",
    "            x_t_sampled[:, :num_prefix_tokens] = prefix_ids_tensor\n",
    "            # Suffix 고정 (시퀀스 끝에서부터)\n",
    "            x_t_sampled[:, infilling_total_sequence_length - num_suffix_tokens:] = suffix_ids_tensor\n",
    "            return x_t_sampled\n",
    "\n",
    "        # --- Infilling 샘플링 함수 가져오기 ---\n",
    "        try:\n",
    "            if hasattr(score_model, 'config') and score_model.config:\n",
    "                predictor_type_infill = score_model.config.sampling.predictor\n",
    "                denoise_sampling_infill = score_model.config.sampling.noise_removal\n",
    "            else:\n",
    "                predictor_type_infill = 'euler'\n",
    "                denoise_sampling_infill = True\n",
    "            \n",
    "            infilling_sampling_fn = sampling.get_pc_sampler(\n",
    "                graph=graph,\n",
    "                noise=noise,\n",
    "                batch_dims=(num_infilling_samples, infilling_total_sequence_length),\n",
    "                predictor=predictor_type_infill,\n",
    "                steps=sampling_steps_infilling,\n",
    "                denoise=denoise_sampling_infill,\n",
    "                eps=sampling_eps_infilling,\n",
    "                device=device,\n",
    "                proj_fun=infilling_proj_fun # Infilling용 projection 함수 전달\n",
    "            )\n",
    "            print(\"Infilling 샘플링 함수 준비 완료.\")\n",
    "\n",
    "            # --- Infilling 텍스트 생성 수행 ---\n",
    "            print(f\"\\nInfilling 텍스트 샘플을 생성합니다 (총 길이: {infilling_total_sequence_length})...\")\n",
    "            \n",
    "            score_model.eval()\n",
    "            with torch.no_grad():\n",
    "                # 초기 노이즈는 전체 시퀀스 길이에 대해 생성됨\n",
    "                # infilling_proj_fun에 의해 prefix와 suffix가 고정되면서 샘플링 진행\n",
    "                generated_infilling_ids = infilling_sampling_fn(score_model)\n",
    "                # 샘플링 후에도 한 번 더 proj_fun을 적용하여 고정 보장\n",
    "                generated_infilling_ids = infilling_proj_fun(generated_infilling_ids)\n",
    "\n",
    "            print(\"Infilling 텍스트 생성 완료.\")\n",
    "\n",
    "            # --- 생성된 텍스트 디코딩 ---\n",
    "            print(f\"\\n--- 생성된 Infilling 텍스트 샘플 ---\")\n",
    "            generated_texts_infilling = tokenizer.batch_decode(generated_infilling_ids, skip_special_tokens=True)\n",
    "            \n",
    "            for i, text in enumerate(generated_texts_infilling):\n",
    "                print(f\"\\n--- 샘플 {i+1} ---\")\n",
    "                print(text)\n",
    "                # 채워진 부분만 따로 보고 싶다면:\n",
    "                # filled_part_ids = generated_infilling_ids[i, num_prefix_tokens : infilling_total_sequence_length - num_suffix_tokens]\n",
    "                # filled_part_text = tokenizer.decode(filled_part_ids, skip_special_tokens=True)\n",
    "                # print(f\"채워진 부분: {filled_part_text}\")\n",
    "                print(\"-\" * 70)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Infilling 샘플링 중 오류 발생: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "else:\n",
    "    print(\"오류: 토크나이저가 로드되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5c565e1-f438-45a7-8c9f-1753a5a2d049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix: '<DUE>2021-05-13 <ITEM>K04033 <COST>25870 <QTY>318 <URGENT>1 //' (35 토큰)\n",
      "Suffix: ' ' (1 토큰)\n",
      "Infilling 샘플링 함수 준비 완료.\n",
      "\n",
      "Infilling 텍스트 샘플을 생성합니다 (총 길이: 64)...\n",
      "Infilling 텍스트 생성 완료.\n",
      "\n",
      "--- 생성된 Infilling 텍스트 샘플 ---\n",
      "\n",
      "--- 샘플 1 ---\n",
      "<DUE>2021-05-13 <ITEM>K04033 <COST>25870 <QTY>318 <URGENT>1 //49759 Jun 15, 2015 <15 < 17> 1 #1 #1 <LAND>3245 -------------- <ID>3330 \n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 셀 9: Infilling (내용 채우기) 방식의 조건부 추론\n",
    "\n",
    "import torch\n",
    "import sampling # Score-Entropy-Discrete-Diffusion 레포 내의 sampling.py\n",
    "# from transformers import GPT2TokenizerFast # 이미 로드되어 있다고 가정 (tokenizer 변수)\n",
    "\n",
    "# --- Infilling 설정 ---\n",
    "prefix_text = \"<DUE>2021-05-13 <ITEM>K04033 <COST>25870 <QTY>318 <URGENT>1 //\"\n",
    "suffix_text = \" \"\n",
    "# 예시: \"The weather today is [모델이 생성한 내용] so I decided to stay home.\"\n",
    "\n",
    "num_infilling_samples = 1   # 생성할 Infilling 샘플 개수\n",
    "infilling_total_sequence_length = 64 # Prefix + 생성부분 + Suffix 를 포함한 전체 시퀀스 길이\n",
    "                                     # 이 길이가 너무 짧으면 prefix/suffix가 겹치거나 생성 공간이 없을 수 있음\n",
    "                                     # 모델의 최대 시퀀스 길이 (예: 1024)를 넘지 않도록 주의\n",
    "sampling_steps_infilling = 128\n",
    "sampling_eps_infilling = 1e-5\n",
    "\n",
    "# --- Prefix 및 Suffix 토큰화 ---\n",
    "if 'tokenizer' in globals() and tokenizer:\n",
    "    prefix_ids = tokenizer(prefix_text, add_special_tokens=False)['input_ids']\n",
    "    suffix_ids = tokenizer(suffix_text, add_special_tokens=False)['input_ids']\n",
    "    \n",
    "    num_prefix_tokens = len(prefix_ids)\n",
    "    num_suffix_tokens = len(suffix_ids)\n",
    "\n",
    "    print(f\"Prefix: '{prefix_text}' ({num_prefix_tokens} 토큰)\")\n",
    "    print(f\"Suffix: '{suffix_text}' ({num_suffix_tokens} 토큰)\")\n",
    "\n",
    "    if num_prefix_tokens + num_suffix_tokens >= infilling_total_sequence_length:\n",
    "        print(\"오류: Prefix와 Suffix의 길이 합이 전체 시퀀스 길이보다 크거나 같습니다. Infilling 공간이 없습니다.\")\n",
    "        # 이 경우 샘플링 중단 또는 파라미터 조정 필요\n",
    "        generated_texts_infilling = [\"오류: Infilling 공간 부족\"]\n",
    "    else:\n",
    "        # --- Infilling을 위한 proj_fun 정의 ---\n",
    "        # 전체 시퀀스에서 prefix와 suffix 위치의 토큰들을 고정\n",
    "        \n",
    "        # 고정될 토큰 ID 텐서 준비 (배치 차원 추가)\n",
    "        # prefix 부분\n",
    "        prefix_ids_tensor = torch.tensor(prefix_ids, device=device).unsqueeze(0).expand(num_infilling_samples, -1)\n",
    "        # suffix 부분\n",
    "        suffix_ids_tensor = torch.tensor(suffix_ids, device=device).unsqueeze(0).expand(num_infilling_samples, -1)\n",
    "\n",
    "        def infilling_proj_fun(x_t_sampled):\n",
    "            # x_t_sampled의 모양: (num_infilling_samples, infilling_total_sequence_length)\n",
    "            \n",
    "            # Prefix 고정\n",
    "            x_t_sampled[:, :num_prefix_tokens] = prefix_ids_tensor\n",
    "            # Suffix 고정 (시퀀스 끝에서부터)\n",
    "            x_t_sampled[:, infilling_total_sequence_length - num_suffix_tokens:] = suffix_ids_tensor\n",
    "            return x_t_sampled\n",
    "\n",
    "        # --- Infilling 샘플링 함수 가져오기 ---\n",
    "        try:\n",
    "            if hasattr(score_model, 'config') and score_model.config:\n",
    "                predictor_type_infill = score_model.config.sampling.predictor\n",
    "                denoise_sampling_infill = score_model.config.sampling.noise_removal\n",
    "            else:\n",
    "                predictor_type_infill = 'euler'\n",
    "                denoise_sampling_infill = True\n",
    "            \n",
    "            infilling_sampling_fn = sampling.get_pc_sampler(\n",
    "                graph=graph,\n",
    "                noise=noise,\n",
    "                batch_dims=(num_infilling_samples, infilling_total_sequence_length),\n",
    "                predictor=predictor_type_infill,\n",
    "                steps=sampling_steps_infilling,\n",
    "                denoise=denoise_sampling_infill,\n",
    "                eps=sampling_eps_infilling,\n",
    "                device=device,\n",
    "                proj_fun=infilling_proj_fun # Infilling용 projection 함수 전달\n",
    "            )\n",
    "            print(\"Infilling 샘플링 함수 준비 완료.\")\n",
    "\n",
    "            # --- Infilling 텍스트 생성 수행 ---\n",
    "            print(f\"\\nInfilling 텍스트 샘플을 생성합니다 (총 길이: {infilling_total_sequence_length})...\")\n",
    "            \n",
    "            score_model.eval()\n",
    "            with torch.no_grad():\n",
    "                # 초기 노이즈는 전체 시퀀스 길이에 대해 생성됨\n",
    "                # infilling_proj_fun에 의해 prefix와 suffix가 고정되면서 샘플링 진행\n",
    "                generated_infilling_ids = infilling_sampling_fn(score_model)\n",
    "                # 샘플링 후에도 한 번 더 proj_fun을 적용하여 고정 보장\n",
    "                generated_infilling_ids = infilling_proj_fun(generated_infilling_ids)\n",
    "\n",
    "            print(\"Infilling 텍스트 생성 완료.\")\n",
    "\n",
    "            # --- 생성된 텍스트 디코딩 ---\n",
    "            print(f\"\\n--- 생성된 Infilling 텍스트 샘플 ---\")\n",
    "            generated_texts_infilling = tokenizer.batch_decode(generated_infilling_ids, skip_special_tokens=True)\n",
    "            \n",
    "            for i, text in enumerate(generated_texts_infilling):\n",
    "                print(f\"\\n--- 샘플 {i+1} ---\")\n",
    "                print(text)\n",
    "                # 채워진 부분만 따로 보고 싶다면:\n",
    "                # filled_part_ids = generated_infilling_ids[i, num_prefix_tokens : infilling_total_sequence_length - num_suffix_tokens]\n",
    "                # filled_part_text = tokenizer.decode(filled_part_ids, skip_special_tokens=True)\n",
    "                # print(f\"채워진 부분: {filled_part_text}\")\n",
    "                print(\"-\" * 70)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Infilling 샘플링 중 오류 발생: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "else:\n",
    "    print(\"오류: 토크나이저가 로드되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "457c2fd2-8e59-4b92-a806-301afc12b5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 다양한 문자열 요소 및 특수 기호 토큰화 테스트 ---\n",
      "\n",
      "--- 테스트: Order information ---\n",
      "원본 텍스트: '• K04033, right? 318? You can do it by May 13th, 2021, and the unit price is 25,870 won. Oh, and the inspection product.'\n",
      "Token IDs: [3581, 509, 36676, 2091, 11, 826, 30, 39320, 30, 921, 460, 466, 340, 416, 1737, 1511, 400, 11, 33448, 11, 290, 262, 4326, 2756, 318, 1679, 11, 46951, 1839, 13, 3966, 11, 290, 262, 15210, 1720, 13]\n",
      "디코딩된 각 토큰: ['•', ' K', '040', '33', ',', ' right', '?', ' 318', '?', ' You', ' can', ' do', ' it', ' by', ' May', ' 13', 'th', ',', ' 2021', ',', ' and', ' the', ' unit', ' price', ' is', ' 25', ',', '870', ' won', '.', ' Oh', ',', ' and', ' the', ' inspection', ' product', '.']\n",
      "생성된 토큰 수: 37\n",
      "\n",
      "--- 테스트: machine information ---\n",
      "원본 텍스트: '• item: S00341, machine: 0410, capacity: 4.6'\n",
      "Token IDs: [3581, 2378, 25, 311, 405, 33660, 11, 4572, 25, 8702, 940, 11, 5339, 25, 604, 13, 21]\n",
      "디코딩된 각 토큰: ['•', ' item', ':', ' S', '00', '341', ',', ' machine', ':', ' 04', '10', ',', ' capacity', ':', ' 4', '.', '6']\n",
      "생성된 토큰 수: 17\n",
      "\n",
      "--- 테스트: output ---\n",
      "원본 텍스트: '• item: K04033, machine: 0405, time: 2021-05-13, quantity: 14, <|endoftext|>'\n",
      "Token IDs: [3581, 2378, 25, 509, 36676, 2091, 11, 4572, 25, 657, 26598, 11, 640, 25, 33448, 12, 2713, 12, 1485, 11, 12040, 25, 1478, 11, 220, 50256]\n",
      "디코딩된 각 토큰: ['•', ' item', ':', ' K', '040', '33', ',', ' machine', ':', ' 0', '405', ',', ' time', ':', ' 2021', '-', '05', '-', '13', ',', ' quantity', ':', ' 14', ',', ' ', '<|endoftext|>']\n",
      "생성된 토큰 수: 26\n"
     ]
    }
   ],
   "source": [
    "# 셀 6: 다양한 데이터 요소 및 특수 기호 토큰화 상세 확인\n",
    "\n",
    "if 'tokenizer' in globals() and tokenizer:\n",
    "    test_strings = {\n",
    "        \"Order information\": \"• K04033, right? 318? You can do it by May 13th, 2021, and the unit price is 25,870 won. Oh, and the inspection product.\",\n",
    "        \"machine information\": \"• item: S00341, machine: 0410, capacity: 4.6\",\n",
    "        \"output\": \"• item: K04033, machine: 0405, time: 2021-05-13, quantity: 14, <|endoftext|>\"\n",
    "    }\n",
    "\n",
    "    print(\"--- 다양한 문자열 요소 및 특수 기호 토큰화 테스트 ---\")\n",
    "    for description, text in test_strings.items():\n",
    "        print(f\"\\n--- 테스트: {description} ---\")\n",
    "        print(f\"원본 텍스트: '{text}'\")\n",
    "        \n",
    "        # 토큰화 (add_special_tokens=False로 하여 순수 텍스트에 대한 결과 확인)\n",
    "        tokenized_output = tokenizer(text, add_special_tokens=False)\n",
    "        input_ids = tokenized_output['input_ids']\n",
    "        decoded_tokens = [tokenizer.decode([token_id]) for token_id in input_ids]\n",
    "        \n",
    "        print(f\"Token IDs: {input_ids}\")\n",
    "        print(f\"디코딩된 각 토큰: {decoded_tokens}\")\n",
    "        print(f\"생성된 토큰 수: {len(input_ids)}\")\n",
    "\n",
    "else:\n",
    "    print(\"오류: 토크나이저가 로드되지 않았습니다. 이전 셀을 먼저 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9e206c0-eeac-4e06-8705-dc9d04b598a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sampledata/dataset_118_eng.csv' 파일 로드 완료. Shape: (118, 2)\n",
      "사용할 구분자 (SEP_TOKEN): '<|endoftext|>'\n",
      "\n",
      "각 샘플 토큰화 진행 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2859 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 샘플 1 ---\n",
      "Conditioning 길이 (문자): 4921\n",
      "Output 길이 (문자): 2647\n",
      "Combined 길이 (문자): 7581\n",
      "Combined 토큰 길이: 2859\n",
      "\n",
      "--- 샘플 2 ---\n",
      "Conditioning 길이 (문자): 4078\n",
      "Output 길이 (문자): 2149\n",
      "Combined 길이 (문자): 6240\n",
      "Combined 토큰 길이: 2342\n",
      "\n",
      "--- 전체 샘플의 Combined 토큰 길이 통계 ---\n",
      "총 분석된 샘플 수: 118\n",
      "최소 토큰 길이: 1395\n",
      "최대 토큰 길이: 3995\n",
      "평균 토큰 길이: 2737.17\n",
      "중앙값 토큰 길이: 2739.5\n",
      "90 백분위수 토큰 길이: 3654.40\n",
      "95 백분위수 토큰 길이: 3805.40\n",
      "99 백분위수 토큰 길이: 3970.54\n",
      "\n",
      "matplotlib이 설치되어 있지 않아 히스토그램을 표시할 수 없습니다.\n",
      "히스토그램을 보려면 'pip install matplotlib' 또는 'conda install matplotlib'을 실행하세요.\n"
     ]
    }
   ],
   "source": [
    "# 셀 10: 새로운 실전 데이터 샘플 분석 (토큰화 후 시퀀스 길이 확인)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from transformers import GPT2TokenizerFast # 이미 로드되어 있다고 가정 (tokenizer 변수)\n",
    "\n",
    "# --- 데이터 파일 경로 ---\n",
    "# 실제 파인튜닝 시에는 'dataset_118_eng.csv'를 사용하게 됩니다.\n",
    "# 여기서는 제공해주신 샘플 파일로 분석합니다.\n",
    "new_data_path = 'sampledata/dataset_118_eng.csv' \n",
    "\n",
    "# --- 데이터 로드 ---\n",
    "try:\n",
    "    df_new_data = pd.read_csv(new_data_path)\n",
    "    print(f\"'{new_data_path}' 파일 로드 완료. Shape: {df_new_data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: CSV 파일을 찾을 수 없습니다. 경로를 확인하세요: {new_data_path}\")\n",
    "    df_new_data = pd.DataFrame() # 오류 시 빈 DataFrame으로\n",
    "\n",
    "# --- 토큰 길이 분석 ---\n",
    "if not df_new_data.empty and 'tokenizer' in globals() and tokenizer:\n",
    "    token_lengths = []\n",
    "    \n",
    "    # 구분자로 사용할 토큰 (문자열 형태)\n",
    "    sep_token_str = tokenizer.eos_token \n",
    "    print(f\"사용할 구분자 (SEP_TOKEN): '{sep_token_str}'\")\n",
    "\n",
    "    print(\"\\n각 샘플 토큰화 진행 중...\")\n",
    "    for index, row in df_new_data.iterrows():\n",
    "        conditioning_text = str(row['conditioning']) # NaN 방지를 위해 str 변환\n",
    "        output_text = str(row['output'])         # NaN 방지를 위해 str 변환\n",
    "        \n",
    "        # \"conditioning_text + <SEP_TOKEN> + output_text\" 형태로 합치기\n",
    "        combined_text = conditioning_text + sep_token_str + output_text\n",
    "        \n",
    "        # 토큰화 (특수 토큰(예: 문장 시작/끝)은 제외하고 순수 텍스트 길이만 계산)\n",
    "        tokenized_output = tokenizer(combined_text, add_special_tokens=False)\n",
    "        token_lengths.append(len(tokenized_output['input_ids']))\n",
    "        \n",
    "        if index < 2: # 처음 2개 샘플에 대해서만 상세 정보 출력\n",
    "            print(f\"\\n--- 샘플 {index+1} ---\")\n",
    "            print(f\"Conditioning 길이 (문자): {len(conditioning_text)}\")\n",
    "            print(f\"Output 길이 (문자): {len(output_text)}\")\n",
    "            print(f\"Combined 길이 (문자): {len(combined_text)}\")\n",
    "            print(f\"Combined 토큰 길이: {len(tokenized_output['input_ids'])}\")\n",
    "            # print(f\"Combined Text (일부): {combined_text[:200]}...\") # 너무 길면 일부만 출력\n",
    "\n",
    "    if token_lengths:\n",
    "        print(\"\\n--- 전체 샘플의 Combined 토큰 길이 통계 ---\")\n",
    "        print(f\"총 분석된 샘플 수: {len(token_lengths)}\")\n",
    "        print(f\"최소 토큰 길이: {np.min(token_lengths)}\")\n",
    "        print(f\"최대 토큰 길이: {np.max(token_lengths)}\")\n",
    "        print(f\"평균 토큰 길이: {np.mean(token_lengths):.2f}\")\n",
    "        print(f\"중앙값 토큰 길이: {np.median(token_lengths)}\")\n",
    "        # 분위수 확인 (예: 90%, 95%, 99%)\n",
    "        print(f\"90 백분위수 토큰 길이: {np.percentile(token_lengths, 90):.2f}\")\n",
    "        print(f\"95 백분위수 토큰 길이: {np.percentile(token_lengths, 95):.2f}\")\n",
    "        print(f\"99 백분위수 토큰 길이: {np.percentile(token_lengths, 99):.2f}\")\n",
    "        \n",
    "        # 간단한 히스토그램 출력 (matplotlib 필요)\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(token_lengths, bins=50, alpha=0.7, color='blue')\n",
    "            plt.title('Distribution of Combined Token Lengths')\n",
    "            plt.xlabel('Token Length')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        except ImportError:\n",
    "            print(\"\\nmatplotlib이 설치되어 있지 않아 히스토그램을 표시할 수 없습니다.\")\n",
    "            print(\"히스토그램을 보려면 'pip install matplotlib' 또는 'conda install matplotlib'을 실행하세요.\")\n",
    "\n",
    "    else:\n",
    "        print(\"토큰 길이를 분석할 데이터가 없습니다.\")\n",
    "        \n",
    "elif df_new_data.empty:\n",
    "    print(f\"오류: '{new_data_path}' 파일이 비어 있거나 로드되지 않았습니다.\")\n",
    "else:\n",
    "    print(\"오류: 토크나이저가 로드되지 않았습니다. 이전 셀을 먼저 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a087d196-2525-47c9-bb1d-d42e61813d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sampledata/dataset_118_eng_sample.csv' 파일 로드 완료. Shape: (1, 2)\n",
      "사용할 구분자 (SEP_TOKEN): '<|endoftext|>'\n",
      "\n",
      "각 샘플 토큰화 진행 중...\n",
      "\n",
      "--- 샘플 1 ---\n",
      "Conditioning 길이 (문자): 1273\n",
      "Output 길이 (문자): 1\n",
      "Combined 길이 (문자): 1287\n",
      "Combined 토큰 길이: 385\n",
      "\n",
      "--- 전체 샘플의 Combined 토큰 길이 통계 ---\n",
      "총 분석된 샘플 수: 1\n",
      "최소 토큰 길이: 385\n",
      "최대 토큰 길이: 385\n",
      "평균 토큰 길이: 385.00\n",
      "중앙값 토큰 길이: 385.0\n",
      "90 백분위수 토큰 길이: 385.00\n",
      "95 백분위수 토큰 길이: 385.00\n",
      "99 백분위수 토큰 길이: 385.00\n",
      "\n",
      "matplotlib이 설치되어 있지 않아 히스토그램을 표시할 수 없습니다.\n",
      "히스토그램을 보려면 'pip install matplotlib' 또는 'conda install matplotlib'을 실행하세요.\n"
     ]
    }
   ],
   "source": [
    "# 셀 10: 새로운 실전 데이터 샘플 분석 (토큰화 후 시퀀스 길이 확인)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from transformers import GPT2TokenizerFast # 이미 로드되어 있다고 가정 (tokenizer 변수)\n",
    "\n",
    "# --- 데이터 파일 경로 ---\n",
    "# 실제 파인튜닝 시에는 'dataset_118_eng.csv'를 사용하게 됩니다.\n",
    "# 여기서는 제공해주신 샘플 파일로 분석합니다.\n",
    "new_data_path = 'sampledata/dataset_118_eng_sample.csv' \n",
    "\n",
    "# --- 데이터 로드 ---\n",
    "try:\n",
    "    df_new_data = pd.read_csv(new_data_path)\n",
    "    print(f\"'{new_data_path}' 파일 로드 완료. Shape: {df_new_data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: CSV 파일을 찾을 수 없습니다. 경로를 확인하세요: {new_data_path}\")\n",
    "    df_new_data = pd.DataFrame() # 오류 시 빈 DataFrame으로\n",
    "\n",
    "# --- 토큰 길이 분석 ---\n",
    "if not df_new_data.empty and 'tokenizer' in globals() and tokenizer:\n",
    "    token_lengths = []\n",
    "    \n",
    "    # 구분자로 사용할 토큰 (문자열 형태)\n",
    "    sep_token_str = tokenizer.eos_token \n",
    "    print(f\"사용할 구분자 (SEP_TOKEN): '{sep_token_str}'\")\n",
    "\n",
    "    print(\"\\n각 샘플 토큰화 진행 중...\")\n",
    "    for index, row in df_new_data.iterrows():\n",
    "        conditioning_text = str(row['conditioning']) # NaN 방지를 위해 str 변환\n",
    "        output_text = str(row['output'])         # NaN 방지를 위해 str 변환\n",
    "        \n",
    "        # \"conditioning_text + <SEP_TOKEN> + output_text\" 형태로 합치기\n",
    "        combined_text = conditioning_text + sep_token_str + output_text\n",
    "        \n",
    "        # 토큰화 (특수 토큰(예: 문장 시작/끝)은 제외하고 순수 텍스트 길이만 계산)\n",
    "        tokenized_output = tokenizer(combined_text, add_special_tokens=False)\n",
    "        token_lengths.append(len(tokenized_output['input_ids']))\n",
    "        \n",
    "        if index < 2: # 처음 2개 샘플에 대해서만 상세 정보 출력\n",
    "            print(f\"\\n--- 샘플 {index+1} ---\")\n",
    "            print(f\"Conditioning 길이 (문자): {len(conditioning_text)}\")\n",
    "            print(f\"Output 길이 (문자): {len(output_text)}\")\n",
    "            print(f\"Combined 길이 (문자): {len(combined_text)}\")\n",
    "            print(f\"Combined 토큰 길이: {len(tokenized_output['input_ids'])}\")\n",
    "            # print(f\"Combined Text (일부): {combined_text[:200]}...\") # 너무 길면 일부만 출력\n",
    "\n",
    "    if token_lengths:\n",
    "        print(\"\\n--- 전체 샘플의 Combined 토큰 길이 통계 ---\")\n",
    "        print(f\"총 분석된 샘플 수: {len(token_lengths)}\")\n",
    "        print(f\"최소 토큰 길이: {np.min(token_lengths)}\")\n",
    "        print(f\"최대 토큰 길이: {np.max(token_lengths)}\")\n",
    "        print(f\"평균 토큰 길이: {np.mean(token_lengths):.2f}\")\n",
    "        print(f\"중앙값 토큰 길이: {np.median(token_lengths)}\")\n",
    "        # 분위수 확인 (예: 90%, 95%, 99%)\n",
    "        print(f\"90 백분위수 토큰 길이: {np.percentile(token_lengths, 90):.2f}\")\n",
    "        print(f\"95 백분위수 토큰 길이: {np.percentile(token_lengths, 95):.2f}\")\n",
    "        print(f\"99 백분위수 토큰 길이: {np.percentile(token_lengths, 99):.2f}\")\n",
    "        \n",
    "        # 간단한 히스토그램 출력 (matplotlib 필요)\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(token_lengths, bins=50, alpha=0.7, color='blue')\n",
    "            plt.title('Distribution of Combined Token Lengths')\n",
    "            plt.xlabel('Token Length')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        except ImportError:\n",
    "            print(\"\\nmatplotlib이 설치되어 있지 않아 히스토그램을 표시할 수 없습니다.\")\n",
    "            print(\"히스토그램을 보려면 'pip install matplotlib' 또는 'conda install matplotlib'을 실행하세요.\")\n",
    "\n",
    "    else:\n",
    "        print(\"토큰 길이를 분석할 데이터가 없습니다.\")\n",
    "        \n",
    "elif df_new_data.empty:\n",
    "    print(f\"오류: '{new_data_path}' 파일이 비어 있거나 로드되지 않았습니다.\")\n",
    "else:\n",
    "    print(\"오류: 토크나이저가 로드되지 않았습니다. 이전 셀을 먼저 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15311c2b-f0a2-4a57-b2b6-4e7e367617f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sampledata/dataset_118_eng_sample.csv' 파일 로드 완료. Shape: (1, 2)\n",
      "사용할 구분자 (SEP_TOKEN): '<|endoftext|>'\n",
      "\n",
      "각 샘플 토큰화 진행 중...\n",
      "\n",
      "--- 샘플 1 ---\n",
      "Conditioning 길이 (문자): 3648\n",
      "Output 길이 (문자): 1\n",
      "Combined 길이 (문자): 3662\n",
      "Combined 토큰 길이: 1439\n",
      "\n",
      "--- 전체 샘플의 Combined 토큰 길이 통계 ---\n",
      "총 분석된 샘플 수: 1\n",
      "최소 토큰 길이: 1439\n",
      "최대 토큰 길이: 1439\n",
      "평균 토큰 길이: 1439.00\n",
      "중앙값 토큰 길이: 1439.0\n",
      "90 백분위수 토큰 길이: 1439.00\n",
      "95 백분위수 토큰 길이: 1439.00\n",
      "99 백분위수 토큰 길이: 1439.00\n",
      "\n",
      "matplotlib이 설치되어 있지 않아 히스토그램을 표시할 수 없습니다.\n",
      "히스토그램을 보려면 'pip install matplotlib' 또는 'conda install matplotlib'을 실행하세요.\n"
     ]
    }
   ],
   "source": [
    "# 셀 10: 새로운 실전 데이터 샘플 분석 (토큰화 후 시퀀스 길이 확인)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from transformers import GPT2TokenizerFast # 이미 로드되어 있다고 가정 (tokenizer 변수)\n",
    "\n",
    "# --- 데이터 파일 경로 ---\n",
    "# 실제 파인튜닝 시에는 'dataset_118_eng.csv'를 사용하게 됩니다.\n",
    "# 여기서는 제공해주신 샘플 파일로 분석합니다.\n",
    "new_data_path = 'sampledata/dataset_118_eng_sample.csv' \n",
    "\n",
    "# --- 데이터 로드 ---\n",
    "try:\n",
    "    df_new_data = pd.read_csv(new_data_path)\n",
    "    print(f\"'{new_data_path}' 파일 로드 완료. Shape: {df_new_data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: CSV 파일을 찾을 수 없습니다. 경로를 확인하세요: {new_data_path}\")\n",
    "    df_new_data = pd.DataFrame() # 오류 시 빈 DataFrame으로\n",
    "\n",
    "# --- 토큰 길이 분석 ---\n",
    "if not df_new_data.empty and 'tokenizer' in globals() and tokenizer:\n",
    "    token_lengths = []\n",
    "    \n",
    "    # 구분자로 사용할 토큰 (문자열 형태)\n",
    "    sep_token_str = tokenizer.eos_token \n",
    "    print(f\"사용할 구분자 (SEP_TOKEN): '{sep_token_str}'\")\n",
    "\n",
    "    print(\"\\n각 샘플 토큰화 진행 중...\")\n",
    "    for index, row in df_new_data.iterrows():\n",
    "        conditioning_text = str(row['conditioning']) # NaN 방지를 위해 str 변환\n",
    "        output_text = str(row['output'])         # NaN 방지를 위해 str 변환\n",
    "        \n",
    "        # \"conditioning_text + <SEP_TOKEN> + output_text\" 형태로 합치기\n",
    "        combined_text = conditioning_text + sep_token_str + output_text\n",
    "        \n",
    "        # 토큰화 (특수 토큰(예: 문장 시작/끝)은 제외하고 순수 텍스트 길이만 계산)\n",
    "        tokenized_output = tokenizer(combined_text, add_special_tokens=False)\n",
    "        token_lengths.append(len(tokenized_output['input_ids']))\n",
    "        \n",
    "        if index < 2: # 처음 2개 샘플에 대해서만 상세 정보 출력\n",
    "            print(f\"\\n--- 샘플 {index+1} ---\")\n",
    "            print(f\"Conditioning 길이 (문자): {len(conditioning_text)}\")\n",
    "            print(f\"Output 길이 (문자): {len(output_text)}\")\n",
    "            print(f\"Combined 길이 (문자): {len(combined_text)}\")\n",
    "            print(f\"Combined 토큰 길이: {len(tokenized_output['input_ids'])}\")\n",
    "            # print(f\"Combined Text (일부): {combined_text[:200]}...\") # 너무 길면 일부만 출력\n",
    "\n",
    "    if token_lengths:\n",
    "        print(\"\\n--- 전체 샘플의 Combined 토큰 길이 통계 ---\")\n",
    "        print(f\"총 분석된 샘플 수: {len(token_lengths)}\")\n",
    "        print(f\"최소 토큰 길이: {np.min(token_lengths)}\")\n",
    "        print(f\"최대 토큰 길이: {np.max(token_lengths)}\")\n",
    "        print(f\"평균 토큰 길이: {np.mean(token_lengths):.2f}\")\n",
    "        print(f\"중앙값 토큰 길이: {np.median(token_lengths)}\")\n",
    "        # 분위수 확인 (예: 90%, 95%, 99%)\n",
    "        print(f\"90 백분위수 토큰 길이: {np.percentile(token_lengths, 90):.2f}\")\n",
    "        print(f\"95 백분위수 토큰 길이: {np.percentile(token_lengths, 95):.2f}\")\n",
    "        print(f\"99 백분위수 토큰 길이: {np.percentile(token_lengths, 99):.2f}\")\n",
    "        \n",
    "        # 간단한 히스토그램 출력 (matplotlib 필요)\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(token_lengths, bins=50, alpha=0.7, color='blue')\n",
    "            plt.title('Distribution of Combined Token Lengths')\n",
    "            plt.xlabel('Token Length')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        except ImportError:\n",
    "            print(\"\\nmatplotlib이 설치되어 있지 않아 히스토그램을 표시할 수 없습니다.\")\n",
    "            print(\"히스토그램을 보려면 'pip install matplotlib' 또는 'conda install matplotlib'을 실행하세요.\")\n",
    "\n",
    "    else:\n",
    "        print(\"토큰 길이를 분석할 데이터가 없습니다.\")\n",
    "        \n",
    "elif df_new_data.empty:\n",
    "    print(f\"오류: '{new_data_path}' 파일이 비어 있거나 로드되지 않았습니다.\")\n",
    "else:\n",
    "    print(\"오류: 토크나이저가 로드되지 않았습니다. 이전 셀을 먼저 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd7752a4-eb9c-446c-adb0-77963dec223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sampledata/dataset_118_eng_sample.csv' 파일 로드 완료. Shape: (1, 2)\n",
      "사용할 구분자 (SEP_TOKEN): '<|endoftext|>'\n",
      "\n",
      "각 샘플 토큰화 진행 중...\n",
      "\n",
      "--- 샘플 1 ---\n",
      "Conditioning 길이 (문자): 19\n",
      "Output 길이 (문자): 2647\n",
      "Combined 길이 (문자): 2679\n",
      "Combined 토큰 길이: 1040\n",
      "\n",
      "--- 전체 샘플의 Combined 토큰 길이 통계 ---\n",
      "총 분석된 샘플 수: 1\n",
      "최소 토큰 길이: 1040\n",
      "최대 토큰 길이: 1040\n",
      "평균 토큰 길이: 1040.00\n",
      "중앙값 토큰 길이: 1040.0\n",
      "90 백분위수 토큰 길이: 1040.00\n",
      "95 백분위수 토큰 길이: 1040.00\n",
      "99 백분위수 토큰 길이: 1040.00\n",
      "\n",
      "matplotlib이 설치되어 있지 않아 히스토그램을 표시할 수 없습니다.\n",
      "히스토그램을 보려면 'pip install matplotlib' 또는 'conda install matplotlib'을 실행하세요.\n"
     ]
    }
   ],
   "source": [
    "# 셀 10: 새로운 실전 데이터 샘플 분석 (토큰화 후 시퀀스 길이 확인)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from transformers import GPT2TokenizerFast # 이미 로드되어 있다고 가정 (tokenizer 변수)\n",
    "\n",
    "# --- 데이터 파일 경로 ---\n",
    "# 실제 파인튜닝 시에는 'dataset_118_eng.csv'를 사용하게 됩니다.\n",
    "# 여기서는 제공해주신 샘플 파일로 분석합니다.\n",
    "new_data_path = 'sampledata/dataset_118_eng_sample.csv' \n",
    "\n",
    "# --- 데이터 로드 ---\n",
    "try:\n",
    "    df_new_data = pd.read_csv(new_data_path)\n",
    "    print(f\"'{new_data_path}' 파일 로드 완료. Shape: {df_new_data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: CSV 파일을 찾을 수 없습니다. 경로를 확인하세요: {new_data_path}\")\n",
    "    df_new_data = pd.DataFrame() # 오류 시 빈 DataFrame으로\n",
    "\n",
    "# --- 토큰 길이 분석 ---\n",
    "if not df_new_data.empty and 'tokenizer' in globals() and tokenizer:\n",
    "    token_lengths = []\n",
    "    \n",
    "    # 구분자로 사용할 토큰 (문자열 형태)\n",
    "    sep_token_str = tokenizer.eos_token \n",
    "    print(f\"사용할 구분자 (SEP_TOKEN): '{sep_token_str}'\")\n",
    "\n",
    "    print(\"\\n각 샘플 토큰화 진행 중...\")\n",
    "    for index, row in df_new_data.iterrows():\n",
    "        conditioning_text = str(row['conditioning']) # NaN 방지를 위해 str 변환\n",
    "        output_text = str(row['output'])         # NaN 방지를 위해 str 변환\n",
    "        \n",
    "        # \"conditioning_text + <SEP_TOKEN> + output_text\" 형태로 합치기\n",
    "        combined_text = conditioning_text + sep_token_str + output_text\n",
    "        \n",
    "        # 토큰화 (특수 토큰(예: 문장 시작/끝)은 제외하고 순수 텍스트 길이만 계산)\n",
    "        tokenized_output = tokenizer(combined_text, add_special_tokens=False)\n",
    "        token_lengths.append(len(tokenized_output['input_ids']))\n",
    "        \n",
    "        if index < 2: # 처음 2개 샘플에 대해서만 상세 정보 출력\n",
    "            print(f\"\\n--- 샘플 {index+1} ---\")\n",
    "            print(f\"Conditioning 길이 (문자): {len(conditioning_text)}\")\n",
    "            print(f\"Output 길이 (문자): {len(output_text)}\")\n",
    "            print(f\"Combined 길이 (문자): {len(combined_text)}\")\n",
    "            print(f\"Combined 토큰 길이: {len(tokenized_output['input_ids'])}\")\n",
    "            # print(f\"Combined Text (일부): {combined_text[:200]}...\") # 너무 길면 일부만 출력\n",
    "\n",
    "    if token_lengths:\n",
    "        print(\"\\n--- 전체 샘플의 Combined 토큰 길이 통계 ---\")\n",
    "        print(f\"총 분석된 샘플 수: {len(token_lengths)}\")\n",
    "        print(f\"최소 토큰 길이: {np.min(token_lengths)}\")\n",
    "        print(f\"최대 토큰 길이: {np.max(token_lengths)}\")\n",
    "        print(f\"평균 토큰 길이: {np.mean(token_lengths):.2f}\")\n",
    "        print(f\"중앙값 토큰 길이: {np.median(token_lengths)}\")\n",
    "        # 분위수 확인 (예: 90%, 95%, 99%)\n",
    "        print(f\"90 백분위수 토큰 길이: {np.percentile(token_lengths, 90):.2f}\")\n",
    "        print(f\"95 백분위수 토큰 길이: {np.percentile(token_lengths, 95):.2f}\")\n",
    "        print(f\"99 백분위수 토큰 길이: {np.percentile(token_lengths, 99):.2f}\")\n",
    "        \n",
    "        # 간단한 히스토그램 출력 (matplotlib 필요)\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(token_lengths, bins=50, alpha=0.7, color='blue')\n",
    "            plt.title('Distribution of Combined Token Lengths')\n",
    "            plt.xlabel('Token Length')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        except ImportError:\n",
    "            print(\"\\nmatplotlib이 설치되어 있지 않아 히스토그램을 표시할 수 없습니다.\")\n",
    "            print(\"히스토그램을 보려면 'pip install matplotlib' 또는 'conda install matplotlib'을 실행하세요.\")\n",
    "\n",
    "    else:\n",
    "        print(\"토큰 길이를 분석할 데이터가 없습니다.\")\n",
    "        \n",
    "elif df_new_data.empty:\n",
    "    print(f\"오류: '{new_data_path}' 파일이 비어 있거나 로드되지 않았습니다.\")\n",
    "else:\n",
    "    print(\"오류: 토크나이저가 로드되지 않았습니다. 이전 셀을 먼저 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60f415cc-7330-45b9-835c-e236a4665b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sampledata/dataset_118_eng_sample.csv' 파일 로드 완료. Shape: (1, 2)\n",
      "사용할 구분자 (SEP_TOKEN): '<|endoftext|>'\n",
      "\n",
      "각 샘플 토큰화 진행 중...\n",
      "\n",
      "--- 샘플 1 ---\n",
      "Conditioning 길이 (문자): 20\n",
      "Output 길이 (문자): 1099\n",
      "Combined 길이 (문자): 1132\n",
      "Combined 토큰 길이: 525\n",
      "\n",
      "--- 전체 샘플의 Combined 토큰 길이 통계 ---\n",
      "총 분석된 샘플 수: 1\n",
      "최소 토큰 길이: 525\n",
      "최대 토큰 길이: 525\n",
      "평균 토큰 길이: 525.00\n",
      "중앙값 토큰 길이: 525.0\n",
      "90 백분위수 토큰 길이: 525.00\n",
      "95 백분위수 토큰 길이: 525.00\n",
      "99 백분위수 토큰 길이: 525.00\n",
      "\n",
      "matplotlib이 설치되어 있지 않아 히스토그램을 표시할 수 없습니다.\n",
      "히스토그램을 보려면 'pip install matplotlib' 또는 'conda install matplotlib'을 실행하세요.\n"
     ]
    }
   ],
   "source": [
    "# 셀 10: 새로운 실전 데이터 샘플 분석 (토큰화 후 시퀀스 길이 확인)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from transformers import GPT2TokenizerFast # 이미 로드되어 있다고 가정 (tokenizer 변수)\n",
    "\n",
    "# --- 데이터 파일 경로 ---\n",
    "# 실제 파인튜닝 시에는 'dataset_118_eng.csv'를 사용하게 됩니다.\n",
    "# 여기서는 제공해주신 샘플 파일로 분석합니다.\n",
    "new_data_path = 'sampledata/dataset_118_eng_sample.csv' \n",
    "\n",
    "# --- 데이터 로드 ---\n",
    "try:\n",
    "    df_new_data = pd.read_csv(new_data_path)\n",
    "    print(f\"'{new_data_path}' 파일 로드 완료. Shape: {df_new_data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: CSV 파일을 찾을 수 없습니다. 경로를 확인하세요: {new_data_path}\")\n",
    "    df_new_data = pd.DataFrame() # 오류 시 빈 DataFrame으로\n",
    "\n",
    "# --- 토큰 길이 분석 ---\n",
    "if not df_new_data.empty and 'tokenizer' in globals() and tokenizer:\n",
    "    token_lengths = []\n",
    "    \n",
    "    # 구분자로 사용할 토큰 (문자열 형태)\n",
    "    sep_token_str = tokenizer.eos_token \n",
    "    print(f\"사용할 구분자 (SEP_TOKEN): '{sep_token_str}'\")\n",
    "\n",
    "    print(\"\\n각 샘플 토큰화 진행 중...\")\n",
    "    for index, row in df_new_data.iterrows():\n",
    "        conditioning_text = str(row['conditioning']) # NaN 방지를 위해 str 변환\n",
    "        output_text = str(row['output'])         # NaN 방지를 위해 str 변환\n",
    "        \n",
    "        # \"conditioning_text + <SEP_TOKEN> + output_text\" 형태로 합치기\n",
    "        combined_text = conditioning_text + sep_token_str + output_text\n",
    "        \n",
    "        # 토큰화 (특수 토큰(예: 문장 시작/끝)은 제외하고 순수 텍스트 길이만 계산)\n",
    "        tokenized_output = tokenizer(combined_text, add_special_tokens=False)\n",
    "        token_lengths.append(len(tokenized_output['input_ids']))\n",
    "        \n",
    "        if index < 2: # 처음 2개 샘플에 대해서만 상세 정보 출력\n",
    "            print(f\"\\n--- 샘플 {index+1} ---\")\n",
    "            print(f\"Conditioning 길이 (문자): {len(conditioning_text)}\")\n",
    "            print(f\"Output 길이 (문자): {len(output_text)}\")\n",
    "            print(f\"Combined 길이 (문자): {len(combined_text)}\")\n",
    "            print(f\"Combined 토큰 길이: {len(tokenized_output['input_ids'])}\")\n",
    "            # print(f\"Combined Text (일부): {combined_text[:200]}...\") # 너무 길면 일부만 출력\n",
    "\n",
    "    if token_lengths:\n",
    "        print(\"\\n--- 전체 샘플의 Combined 토큰 길이 통계 ---\")\n",
    "        print(f\"총 분석된 샘플 수: {len(token_lengths)}\")\n",
    "        print(f\"최소 토큰 길이: {np.min(token_lengths)}\")\n",
    "        print(f\"최대 토큰 길이: {np.max(token_lengths)}\")\n",
    "        print(f\"평균 토큰 길이: {np.mean(token_lengths):.2f}\")\n",
    "        print(f\"중앙값 토큰 길이: {np.median(token_lengths)}\")\n",
    "        # 분위수 확인 (예: 90%, 95%, 99%)\n",
    "        print(f\"90 백분위수 토큰 길이: {np.percentile(token_lengths, 90):.2f}\")\n",
    "        print(f\"95 백분위수 토큰 길이: {np.percentile(token_lengths, 95):.2f}\")\n",
    "        print(f\"99 백분위수 토큰 길이: {np.percentile(token_lengths, 99):.2f}\")\n",
    "        \n",
    "        # 간단한 히스토그램 출력 (matplotlib 필요)\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(token_lengths, bins=50, alpha=0.7, color='blue')\n",
    "            plt.title('Distribution of Combined Token Lengths')\n",
    "            plt.xlabel('Token Length')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        except ImportError:\n",
    "            print(\"\\nmatplotlib이 설치되어 있지 않아 히스토그램을 표시할 수 없습니다.\")\n",
    "            print(\"히스토그램을 보려면 'pip install matplotlib' 또는 'conda install matplotlib'을 실행하세요.\")\n",
    "\n",
    "    else:\n",
    "        print(\"토큰 길이를 분석할 데이터가 없습니다.\")\n",
    "        \n",
    "elif df_new_data.empty:\n",
    "    print(f\"오류: '{new_data_path}' 파일이 비어 있거나 로드되지 않았습니다.\")\n",
    "else:\n",
    "    print(\"오류: 토크나이저가 로드되지 않았습니다. 이전 셀을 먼저 실행해주세요.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
