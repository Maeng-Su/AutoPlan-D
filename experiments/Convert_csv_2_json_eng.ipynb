{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b062140-4191-4fd0-b366-62566a81a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# INPUT_FILE = './data/split_54_20250614.csv'\n",
    "INPUT_FILE = './gt_n5_20250618_divide_5_new_deduplicated.csv'\n",
    "# OUTPUT_FILE = './data/llada_format/split_54_0614_tojson_eng.json'\n",
    "OUTPUT_FILE = './data/llada_format/gt_n5_20250618_new_tojson_eng.json'\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    order = row['order']  # 문자열\n",
    "    machine_raw = row['machine']\n",
    "    gt_raw = row['gt']\n",
    "\n",
    "    # 1) order에서 날짜, 품목, 수량 등 추출\n",
    "    order_parts = order.split()\n",
    "    date = order_parts[1]\n",
    "    item = order_parts[2]\n",
    "    cost = order_parts[3]\n",
    "    urgent = order_parts[4]\n",
    "    qty = order_parts[5]\n",
    "\n",
    "    # 2) machine, gt 멀티라인 텍스트 → 리스트 파싱\n",
    "    machines = []\n",
    "    for line in machine_raw.split('\\n'):\n",
    "        if line.strip():\n",
    "            # '• machine24 5.0' → ['machine24', '5.0']\n",
    "            parts = line.strip('• ').split()\n",
    "            machines.append((parts[0], parts[1]))\n",
    "\n",
    "    gts = []\n",
    "    for line in str(gt_raw).split('\\n'):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if not line:\n",
    "            gts.append(('nan', 'nan'))  # 의도적으로 빈 줄도 처리\n",
    "            continue\n",
    "        \n",
    "        if line.lower() == 'nan':\n",
    "            gts.append(('nan', 'nan'))\n",
    "            continue\n",
    "        \n",
    "        parts = line.lstrip('• ').split()\n",
    "        if len(parts) >= 2:\n",
    "            gts.append((parts[0], parts[1]))\n",
    "        else:\n",
    "            gts.append(('nan', 'nan'))  # 불완전한 정보도 의도적 처리 가능\n",
    "\n",
    "\n",
    "    # 3) 자연어 문장 생성\n",
    "    input_text = f\"Delivery date is {date}, item is {item}, quantity is {qty}, cost per unit is {cost}, urgent quantity is {urgent}. Available machines and processable quantities are\"\n",
    "    input_text += \", \".join([f\"{m[0]}: {m[1]}\" for m in machines]) + \".\"\n",
    "\n",
    "    output_text = \"Assigned machines are \" + \", \".join([f\"{g[0]}: {g[1]}\" for g in gts]) + \".\"\n",
    "\n",
    "    data_list.append({\"input\": input_text, \"output\": output_text})\n",
    "\n",
    "# 4) JSON 저장\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_list, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6231c96-e4fe-432b-bdb1-f7b3ee36fe3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07e90313-54e8-41e2-9ec3-f7a16251fa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 파일 생성이 완료되었습니다. 총 26개의 데이터가 생성되었습니다.\n",
      "파일 위치: ./data/test128_divide5_0618_final-Copy.json\n"
     ]
    }
   ],
   "source": [
    "# test json 파일 생성 셀\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. 입력 파일 읽기\n",
    "try:\n",
    "    df = pd.read_csv('./data/ori_divide5_final.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"오류: './data/aug10_test_final.csv' 파일을 찾을 수 없습니다.\")\n",
    "    print(\"파일 경로를 확인해주세요.\")\n",
    "    # 파일이 없으면 더 이상 진행하지 않음\n",
    "    # ipynb 환경이므로 빈 데이터프레임을 만들어 아래 코드 실행 시 에러가 나지 않도록 함\n",
    "    df = pd.DataFrame(columns=['order', 'machine', 'gt'])\n",
    "\n",
    "data_list = []\n",
    "\n",
    "# 2. 데이터프레임의 각 행을 순회하며 데이터 변환\n",
    "# enumerate를 사용하여 행 번호(인덱스)를 함께 가져옵니다.\n",
    "for idx, row in df.iterrows():\n",
    "    # 3. 날짜 생성 (1번 행 -> date401, 2번 행 -> date402, ...)\n",
    "    # idx는 0부터 시작하므로 +401을 해줍니다.\n",
    "    current_date = f\"date{idx + 401}\"\n",
    "\n",
    "    # 4. input 텍스트 생성\n",
    "    order_raw = row['order']\n",
    "    input_sentences = []\n",
    "    \n",
    "    # order 컬럼이 비어있지 않고, 문자열일 경우에만 처리\n",
    "    if pd.notna(order_raw) and isinstance(order_raw, str):\n",
    "        order_lines = order_raw.strip().split('\\n')\n",
    "        for i, line in enumerate(order_lines):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # '•' 문자 제거 후 공백으로 분리\n",
    "            parts = line.lstrip('• ').split()\n",
    "            \n",
    "            # 파싱된 데이터가 최소 4개(기존날짜, 품목, 가격, 긴급, 수량 이므로 5개여야함) 이상인지 확인\n",
    "            if len(parts) >= 5:\n",
    "                # order_parts = [기존날짜, 품목, 가격, 긴급, 수량]\n",
    "                item = parts[1]\n",
    "                cost = parts[2]\n",
    "                urgent = parts[3]\n",
    "                qty = parts[4]\n",
    "\n",
    "                if i == 0:\n",
    "                    # 첫 번째 주문 문장\n",
    "                    sentence = f\"Delivery date is {current_date}, item is {item}, quantity is {qty}, cost per unit is {cost}, urgent quantity is {urgent}.\"\n",
    "                else:\n",
    "                    # 두 번째 주문부터의 문장\n",
    "                    sentence = f\" item is {item}, quantity is {qty}, cost per unit is {cost}, urgent quantity is {urgent}.\"\n",
    "                input_sentences.append(sentence)\n",
    "\n",
    "    # 생성된 문장들을 하나로 합침\n",
    "    input_text = \"\".join(input_sentences)\n",
    "\n",
    "    # 5. output 텍스트 생성\n",
    "    gt_raw = row['gt']\n",
    "    gt_groups = defaultdict(list)\n",
    "    \n",
    "    # gt 컬럼이 비어있지 않고, 문자열일 경우에만 처리\n",
    "    if pd.notna(gt_raw) and isinstance(gt_raw, str):\n",
    "        gt_lines = gt_raw.strip().split('\\n')\n",
    "        for line in gt_lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # '•' 문자 제거 후 공백으로 분리\n",
    "            parts = line.lstrip('• ').split()\n",
    "            \n",
    "            # 파싱된 데이터가 최소 4개(품목, 기계, 기존날짜, 수량) 이상인지 확인\n",
    "            if len(parts) >= 4:\n",
    "                # gt_parts = [품목, 기계, 기존날짜, 수량]\n",
    "                item = parts[0]\n",
    "                machine = parts[1]\n",
    "                qty = parts[3]\n",
    "                gt_groups[item].append(f\"{machine}: {qty}\")\n",
    "\n",
    "    output_sentences = []\n",
    "    for item, assignments in gt_groups.items():\n",
    "        assignments_str = \", \".join(assignments)\n",
    "        sentence = f\"For {item} Assigned machines are, {assignments_str}.\"\n",
    "        output_sentences.append(sentence)\n",
    "        \n",
    "    # 생성된 문장들을 공백 한 칸으로 연결하여 합침\n",
    "    output_text = \" \".join(output_sentences)\n",
    "\n",
    "    # 6. 최종 데이터 리스트에 추가\n",
    "    # input_text나 output_text가 비어있지 않은 경우에만 추가 (의미 없는 데이터 방지)\n",
    "    if input_text and output_text:\n",
    "        data_list.append({\"input\": input_text, \"output\": output_text})\n",
    "\n",
    "# 7. JSON 파일로 저장\n",
    "# 디렉터리가 존재하지 않을 경우를 대비하여 생성하는 로직을 추가할 수 있습니다.\n",
    "# 여기서는 './data/' 디렉터리가 이미 존재한다고 가정합니다.\n",
    "output_file_path = './data/test128_divide5_0618_final-Copy.json'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"JSON 파일 생성이 완료되었습니다. 총 {len(data_list)}개의 데이터가 생성되었습니다.\")\n",
    "print(f\"파일 위치: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d2e452-46a0-44a0-ae01-eb460d4c7979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'GSAI-ML/LLaDA-8B-Instruct' 토크나이저를 로드합니다...\n",
      "토크나이저 로드 완료. 특수 토큰을 정의했습니다.\n",
      "분석할 파일: './data/test128_divide5_0618_final.json'\n",
      "총 26개의 데이터를 로드했습니다. 토큰 길이 계산을 시작합니다...\n",
      "토큰 길이 계산 완료.\n",
      "\n",
      "--- 토큰 길이 통계 ---\n",
      "       input_length  output_length  combined_length\n",
      "count         26.00          26.00            26.00\n",
      "mean         241.00         327.58           580.58\n",
      "std           36.82          98.23           114.09\n",
      "min          115.00         134.00           328.00\n",
      "25%          254.00         255.25           523.00\n",
      "50%          255.00         348.00           599.00\n",
      "75%          256.00         402.50           662.50\n",
      "max          258.00         504.00           772.00\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. 설정 ---\n",
    "# 분석할 JSON 파일 경로\n",
    "json_file_path = './data/test128_divide5_0618_final.json'\n",
    "# 사용할 토크나이저 모델 이름\n",
    "model_name = \"GSAI-ML/LLaDA-8B-Instruct\"\n",
    "\n",
    "print(f\"'{model_name}' 토크나이저를 로드합니다...\")\n",
    "\n",
    "# --- 2. 토크나이저 로드 ---\n",
    "try:\n",
    "    # 이전 셀에서 이미 토크나이저가 로드되었을 수 있지만,\n",
    "    # 이 셀만 독립적으로 실행할 수 있도록 다시 로드합니다.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "except Exception as e:\n",
    "    print(f\"토크나이저 로드 중 오류가 발생했습니다: {e}\")\n",
    "    print(\"이전 셀에서 'transformers' 라이브러리가 올바르게 설치되고 모델이 다운로드되었는지 확인해주세요.\")\n",
    "    # 오류 발생 시 진행을 멈춤\n",
    "    tokenizer = None\n",
    "\n",
    "if tokenizer:\n",
    "    # --- 3. 프롬프트 형식에 필요한 특수 토큰 정의 ---\n",
    "    # llada_lora-DAY.ipynb의 형식과 동일하게 정의\n",
    "    bos = tokenizer.bos_token or \"<|startoftext|>\"\n",
    "    eos = tokenizer.eos_token or \"<|endoftext|>\"\n",
    "    start_user = \"<role>user</role>\\n\"\n",
    "    start_assistant = \"<role>assistant</role>\\n\"\n",
    "\n",
    "    print(\"토크나이저 로드 완료. 특수 토큰을 정의했습니다.\")\n",
    "    print(f\"분석할 파일: '{json_file_path}'\")\n",
    "\n",
    "    # --- 4. JSON 파일 로드 및 토큰 길이 계산 ---\n",
    "    token_lengths = []\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            data_list = json.load(f)\n",
    "\n",
    "        print(f\"총 {len(data_list)}개의 데이터를 로드했습니다. 토큰 길이 계산을 시작합니다...\")\n",
    "\n",
    "        for item in data_list:\n",
    "            input_text = item.get('input', '')\n",
    "            output_text = item.get('output', '')\n",
    "\n",
    "            # 프롬프트 형식으로 합친 텍스트\n",
    "            combined_text = f\"{bos}{start_user}{input_text}\\n{start_assistant}{output_text}{eos}\"\n",
    "\n",
    "            # 각 부분 토큰화 및 길이 계산\n",
    "            input_len = len(tokenizer.encode(input_text))\n",
    "            output_len = len(tokenizer.encode(output_text))\n",
    "            combined_len = len(tokenizer.encode(combined_text))\n",
    "\n",
    "            token_lengths.append({\n",
    "                'input_length': input_len,\n",
    "                'output_length': output_len,\n",
    "                'combined_length': combined_len\n",
    "            })\n",
    "        \n",
    "        print(\"토큰 길이 계산 완료.\")\n",
    "\n",
    "        # --- 5. Pandas 데이터프레임으로 변환 및 통계 출력 ---\n",
    "        if token_lengths:\n",
    "            df_lengths = pd.DataFrame(token_lengths)\n",
    "            \n",
    "            print(\"\\n--- 토큰 길이 통계 ---\")\n",
    "            # describe()를 사용하여 통계 정보 출력\n",
    "            # 더 잘 보이도록 포맷팅 설정\n",
    "            pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "            print(df_lengths.describe())\n",
    "        else:\n",
    "            print(\"분석할 데이터가 없습니다.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: '{json_file_path}' 파일을 찾을 수 없습니다.\")\n",
    "        print(\"이전 단계에서 JSON 파일이 정상적으로 생성되었는지, 파일 경로가 올바른지 확인해주세요.\")\n",
    "    except Exception as e:\n",
    "        print(f\"파일 처리 또는 토큰화 중 오류가 발생했습니다: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aidall_3_9_7)",
   "language": "python",
   "name": "aidall_3_9_7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
