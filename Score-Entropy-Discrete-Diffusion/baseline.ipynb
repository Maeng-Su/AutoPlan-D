{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3598f-87ee-4388-af0f-f77827002d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment.yml 의존성 문제로 파일 내용 변경됨\n",
    "# 아래 내용 터미널에서 실행\n",
    "'''\n",
    "cd Score-Entropy-Discrete-Diffusion\n",
    "conda env create -f environment.yml\n",
    "conda activate sedd\n",
    "# (sedd) 환경에서 실행\n",
    "pip install \\\n",
    "  accelerate==0.27.2 \\\n",
    "  aiohttp==3.9.3 \\\n",
    "  aiosignal==1.3.1 \\\n",
    "  antlr4-python3-runtime==4.9.3 \\\n",
    "  appdirs==1.4.4 \\\n",
    "  async-timeout==4.0.3 \\\n",
    "  attrs==23.2.0 \\\n",
    "  beartype==0.14.1 \\\n",
    "  better-abc==0.0.3 \\\n",
    "  certifi==2022.12.7 \\\n",
    "  charset-normalizer==2.1.1 \\\n",
    "  click==8.1.7 \\\n",
    "  cloudpickle==3.0.0 \\\n",
    "  cmake==3.25.0 \\\n",
    "  datasets==2.17.1 \\\n",
    "  dill==0.3.8 \\\n",
    "  docker-pycreds==0.4.0 \\\n",
    "  einops==0.7.0 \\\n",
    "  fancy-einsum==0.0.3 \\\n",
    "  filelock==3.9.0 \\\n",
    "  frozenlist==1.4.1 \\\n",
    "  fsspec==2023.10.0 \\\n",
    "  gitdb==4.0.11 \\\n",
    "  gitpython==3.1.42 \\\n",
    "  huggingface-hub==0.21.1 \\\n",
    "  hydra-core==1.3.2 \\\n",
    "  hydra-submitit-launcher==1.2.0 \\\n",
    "  idna==3.4 \\\n",
    "  jaxtyping==0.2.25 \\\n",
    "  jinja2==3.1.2 \\\n",
    "  lit==15.0.7 \\\n",
    "  markdown-it-py==3.0.0 \\\n",
    "  markupsafe==2.1.3 \\\n",
    "  mdurl==0.1.2 \\\n",
    "  mpmath==1.3.0 \\\n",
    "  multidict==6.0.5 \\\n",
    "  multiprocess==0.70.16 \\\n",
    "  networkx==3.2.1 \\\n",
    "  ninja==1.11.1.1 \\\n",
    "  numpy==1.24.1 \\\n",
    "  omegaconf==2.3.0 \\\n",
    "  pandas==2.2.1 \\\n",
    "  pillow==10.2.0 \\\n",
    "  protobuf==4.25.3 \\\n",
    "  psutil==5.9.8 \\\n",
    "  pyarrow==15.0.0 \\\n",
    "  pyarrow-hotfix==0.6 \\\n",
    "  pygments==2.17.2 \\\n",
    "  python-dateutil==2.8.2 \\\n",
    "  pytz==2024.1 \\\n",
    "  pyyaml==6.0.1 \\\n",
    "  regex==2023.12.25 \\\n",
    "  requests==2.28.1 \\\n",
    "  rich==13.7.0 \\\n",
    "  safetensors==0.4.2 \\\n",
    "  sentry-sdk==1.40.6 \\\n",
    "  setproctitle==1.3.3 \\\n",
    "  smmap==5.0.1 \\\n",
    "  submitit==1.5.1 \\\n",
    "  sympy==1.12 \\\n",
    "  tokenizers==0.15.2 \\\n",
    "  tqdm==4.66.2 \\\n",
    "  transformer-lens==1.14.0 \\\n",
    "  transformers==4.38.1 \\\n",
    "  triton==2.0.0 \\\n",
    "  typeguard==2.13.3 \\\n",
    "  typing-extensions==4.8.0 \\\n",
    "  tzdata==2024.1 \\\n",
    "  urllib3==1.26.13 \\\n",
    "  wandb==0.16.3 \\\n",
    "  xxhash==3.4.1 \\\n",
    "  yarl==1.9.4\n",
    "pip install flash-attn==2.2.2\n",
    "python -m ipykernel install --user --name sedd_env --display-name \"Python (sedd_env)\"\n",
    "'''\n",
    "# 주피터노트북 커널 sedd_env 로 변경 후 아래 셀들 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acd8001b-4a22-4b05-8dbb-9505a13f77ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial working directory: /workspace/Score-Entropy-Discrete-Diffusion\n",
      "라이브러리 임포트 및 경로 설정 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/sedd/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 셀 1: 필요한 라이브러리 임포트 및 경로 설정\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 클론한 레포지토리의 경로를 sys.path에 추가\n",
    "# 현재 Notebook 파일(.ipynb)이 Score-Entropy-Discrete-Diffusion 폴더 내에 있다면\n",
    "# 그리고 터미널에서 해당 폴더로 이동 후 jupyter notebook을 실행했다면,\n",
    "# 아래 os.chdir 부분은 필요 없을 수 있습니다.\n",
    "# 현재 작업 디렉토리를 확인하고, 레포지토리 루트 디렉토리로 설정합니다.\n",
    "print(f\"Initial working directory: {os.getcwd()}\")\n",
    "if not os.path.basename(os.getcwd()) == 'Score-Entropy-Discrete-Diffusion':\n",
    "    # 만약 현재 디렉토리가 레포지토리 루트가 아니라면,\n",
    "    # 사용자님의 실제 클론 경로에 맞게 수정해야 할 수 있습니다.\n",
    "    # 예: /workspace/Score-Entropy-Discrete-Diffusion\n",
    "    # 이 코드는 현재 스크립트가 있는 위치를 기준으로 상위 디렉토리로 이동하는 방식은 아닙니다.\n",
    "    # 가장 확실한 방법은 절대 경로를 사용하거나,\n",
    "    # Jupyter Notebook을 Score-Entropy-Discrete-Diffusion 폴더 내에서 실행하는 것입니다.\n",
    "    # 아래는 일반적인 RunPod 환경을 가정한 예시 경로입니다.\n",
    "    repo_path = '/workspace/Score-Entropy-Discrete-Diffusion'\n",
    "    if os.path.exists(repo_path) and os.path.isdir(repo_path):\n",
    "        os.chdir(repo_path)\n",
    "        print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(f\"Warning: Repository path '{repo_path}' not found. Make sure model files are accessible.\")\n",
    "\n",
    "# sys.path에 현재 디렉토리 (레포지토리 루트) 추가\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import torch\n",
    "# load_model.py가 현재 작업 디렉토리에 있으므로 바로 임포트 가능\n",
    "from load_model import load_model\n",
    "\n",
    "print(\"라이브러리 임포트 및 경로 설정 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2989f1-5224-4e11-81bb-16390a30a8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model from: louaaron/sedd-small\n",
      "\n",
      "Model loaded successfully!\n",
      "Model type: <class 'model.transformer.SEDD'>\n",
      "Graph type: <class 'graph_lib.Absorbing'>\n",
      "Noise type: <class 'noise_lib.LogLinearNoise'>\n"
     ]
    }
   ],
   "source": [
    "# 셀 2: SEDD 모델 로드\n",
    "try:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Hugging Face Hub에서 모델 로드 시도\n",
    "    model_path = \"louaaron/sedd-small\"\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "\n",
    "    # load_model.py의 load_model 함수 호출 (내부적으로 load_model_hf 우선 시도)\n",
    "    score_model, graph, noise = load_model(model_path, device)\n",
    "\n",
    "    print(\"\\nModel loaded successfully!\")\n",
    "    print(f\"Model type: {type(score_model)}\")\n",
    "    # score_model이 DDP로 감싸져 있을 수 있으므로 .module로 실제 모델 접근\n",
    "    if hasattr(score_model, 'module'):\n",
    "        print(f\"Actual model type (inside DDP or similar): {type(score_model.module)}\")\n",
    "    print(f\"Graph type: {type(graph)}\")\n",
    "    print(f\"Noise type: {type(noise)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model loading: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08db012b-8961-4648-8cf7-75e2d31195a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일 로드 완료.\n",
      "order_df shape: (127, 5)\n",
      "machine_df shape: (21151, 3)\n",
      "\n",
      "총 127개의 주문 데이터 전처리 완료.\n",
      "\n",
      "--- 전처리된 데이터 샘플 (첫 2개) ---\n",
      "\n",
      "--- 샘플 1 ---\n",
      "Input Text Components: {'<DUE>': '2021-05-13', '<ITEM>': 'K04033', '<COST>': '25870', '<QTY>': '318', '<URGENT>': '1'}\n",
      "Conditioning Vector: {'K04033': {'404': 12.84, '405': 10.42, '407': 8.69, '408': 9.12, '409': 10.33, '410': 8.99, '412': 12.32, '416': 11.23, '422': 4.0, '424': 9.87, '426': 11.77, '433': 9.54, '434': 13.05, '435': 9.0, '436': 9.0, '438': 4.4, '439': 12.47, '440': 9.4}}\n",
      "Target Vector: {'output_text': '더미 GT 결과: 생산 계획 최적화 완료.'}\n",
      "\n",
      "--- 샘플 2 ---\n",
      "Input Text Components: {'<DUE>': '2021-05-24', '<ITEM>': 'K04031', '<COST>': '16229', '<QTY>': '383', '<URGENT>': '1'}\n",
      "Conditioning Vector: {'K04031': {'407': 4.85, '408': 3.83, '409': 5.37, '410': 3.09, '416': 2.73, '424': 3.94, '425': 5.47, '426': 6.25, '433': 3.37, '434': 3.86, '435': 3.8, '436': 4.53, '440': 3.85}}\n",
      "Target Vector: {'output_text': '더미 GT 결과: 생산 계획 최적화 완료.'}\n"
     ]
    }
   ],
   "source": [
    "# 셀 3: 데이터 로드 및 전처리\n",
    "import pandas as pd\n",
    "import numpy as np # NaN 비교 등을 위해 사용\n",
    "\n",
    "# --- 데이터 파일 경로 ---\n",
    "# 실제 파일 경로로 수정해주세요.\n",
    "# 예시: 현재 디렉토리에 있다면 그대로 사용\n",
    "order_info_path = 'sampledata/order_info.csv'\n",
    "machine_info_path = 'sampledata/machine_info.csv' # 이전에 machine_info_sample.csv를 사용하셨다면, 실제 파일로 변경\n",
    "\n",
    "# --- 데이터 로드 ---\n",
    "try:\n",
    "    order_df = pd.read_csv(order_info_path)\n",
    "    machine_df = pd.read_csv(machine_info_path)\n",
    "    print(\"CSV 파일 로드 완료.\")\n",
    "    print(f\"order_df shape: {order_df.shape}\")\n",
    "    print(f\"machine_df shape: {machine_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: CSV 파일을 찾을 수 없습니다. 경로를 확인하세요: {order_info_path} 또는 {machine_info_path}\")\n",
    "    # 이 경우, 이후 코드 실행이 어려우므로 중단하거나 파일 경로를 수정해야 합니다.\n",
    "    order_df = pd.DataFrame() # 빈 DataFrame으로 초기화하여 에러 방지 (실제 사용시에는 파일 필요)\n",
    "    machine_df = pd.DataFrame()\n",
    "\n",
    "# --- 전처리 함수 정의 ---\n",
    "\n",
    "def preprocess_urgent(value):\n",
    "    \"\"\" '선급' 컬럼 값을 URGENT 값으로 매핑 \"\"\"\n",
    "    if pd.isna(value) or value == '': # NaN 또는 빈 문자열인 경우\n",
    "        return 0\n",
    "    elif value == '검사품':\n",
    "        return 1\n",
    "    else: # 그 외 모든 문자열 값\n",
    "        return 2\n",
    "\n",
    "def create_input_text_components(row):\n",
    "    \"\"\" DataFrame row에서 input_text 구성요소를 생성 \"\"\"\n",
    "    due_date = row['영업납기']\n",
    "    item_code = row['중산도면']\n",
    "    cost = int(row['단가']) # 정수형으로 변환\n",
    "    qty = int(row['수량'])   # 정수형으로 변환\n",
    "    urgent_value = preprocess_urgent(row['선급'])\n",
    "\n",
    "    # <URGENT> 태그를 포함한 구조화된 텍스트의 구성요소 딕셔너리\n",
    "    # 나중에 이 딕셔너리를 바탕으로 실제 <TAG>VALUE 형태의 문자열을 만듭니다.\n",
    "    components = {\n",
    "        \"<DUE>\": str(due_date),\n",
    "        \"<ITEM>\": str(item_code),\n",
    "        \"<COST>\": str(cost),\n",
    "        \"<QTY>\": str(qty),\n",
    "        \"<URGENT>\": str(urgent_value)\n",
    "    }\n",
    "    return components\n",
    "\n",
    "def create_conditioning_vector(item_code, machine_df):\n",
    "    \"\"\" 특정 item_code에 대한 conditioning_vector 생성 \"\"\"\n",
    "    # machine_df에서 해당 item_code를 가진 machine 정보 필터링\n",
    "    relevant_machines = machine_df[machine_df['item'] == item_code]\n",
    "    \n",
    "    condition_dict = {}\n",
    "    if not relevant_machines.empty:\n",
    "        # machine_df의 'machine'은 float으로 읽힐 수 있으므로 str으로 변환 고려\n",
    "        # 또한, 'capacity'도 숫자형이어야 함\n",
    "        condition_dict[str(item_code)] = {\n",
    "            str(int(m_row['machine'])) if pd.notna(m_row['machine']) else str(m_row['machine']): float(m_row['capacity'])\n",
    "            for _, m_row in relevant_machines.iterrows()\n",
    "        }\n",
    "    else:\n",
    "        # 해당 item_code에 대한 기계 정보가 없으면 빈 딕셔너리 또는 특정 표시\n",
    "        condition_dict[str(item_code)] = {} \n",
    "        \n",
    "    return condition_dict\n",
    "\n",
    "def create_target_vector():\n",
    "    \"\"\" 더미 target_vector 생성 \"\"\"\n",
    "    # 현재는 간단한 더미 텍스트로 설정\n",
    "    # 추후 실제 GT 형식에 맞게 수정 필요\n",
    "    return {\"output_text\": \"더미 GT 결과: 생산 계획 최적화 완료.\"}\n",
    "\n",
    "\n",
    "# --- 최종 데이터 구조 생성 ---\n",
    "processed_data_list = []\n",
    "\n",
    "if not order_df.empty:\n",
    "    for index, row in order_df.iterrows():\n",
    "        input_components = create_input_text_components(row)\n",
    "        item_code_for_conditioning = row['중산도면'] # conditioning vector 생성 시 사용할 item 코드\n",
    "        conditioning_vec = create_conditioning_vector(item_code_for_conditioning, machine_df)\n",
    "        target_vec = create_target_vector()\n",
    "        \n",
    "        processed_data_list.append({\n",
    "            \"input_text_components\": input_components, # 실제 input_text 문자열은 다음 단계에서 조립\n",
    "            \"conditioning_vector\": conditioning_vec,\n",
    "            \"target_vector\": target_vec\n",
    "        })\n",
    "    print(f\"\\n총 {len(processed_data_list)}개의 주문 데이터 전처리 완료.\")\n",
    "\n",
    "    # 결과 확인 (첫 2개 데이터 샘플 출력)\n",
    "    if len(processed_data_list) > 0:\n",
    "        print(\"\\n--- 전처리된 데이터 샘플 (첫 2개) ---\")\n",
    "        for i, data_sample in enumerate(processed_data_list[:2]):\n",
    "            print(f\"\\n--- 샘플 {i+1} ---\")\n",
    "            print(f\"Input Text Components: {data_sample['input_text_components']}\")\n",
    "            print(f\"Conditioning Vector: {data_sample['conditioning_vector']}\")\n",
    "            print(f\"Target Vector: {data_sample['target_vector']}\")\n",
    "    else:\n",
    "        print(\"처리된 데이터가 없습니다.\")\n",
    "else:\n",
    "    print(\"order_df가 비어있어 전처리를 수행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b83539-2daa-4d79-bd26-4d75856e4227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 토크나이저 로드 완료.\n",
      "\n",
      "총 127개의 최종 데이터 샘플 생성 완료.\n",
      "\n",
      "--- 토큰화할 텍스트 샘플 ---\n",
      "<DUE>2021-05-13 <ITEM>K04033 <COST>25870 <QTY>318 <URGENT>1\n",
      "\n",
      "--- 토큰화 결과 (Token IDs) ---\n",
      "[27, 35, 8924, 29, 1238, 2481, 12, 2713, 12, 1485, 1279, 2043, 3620, 29, 42, 36676, 2091, 1279, 8220, 2257, 29, 25600, 2154, 1279, 48, 9936, 29, 36042, 1279, 4261, 38, 3525, 29, 16]\n",
      "\n",
      "--- 디코딩된 각 토큰 ---\n",
      "['<', 'D', 'UE', '>', '20', '21', '-', '05', '-', '13', ' <', 'IT', 'EM', '>', 'K', '040', '33', ' <', 'CO', 'ST', '>', '258', '70', ' <', 'Q', 'TY', '>', '318', ' <', 'UR', 'G', 'ENT', '>', '1']\n",
      "\n",
      "--- 토크나이저 어휘 크기 (Vocabulary Size) ---\n",
      "50257\n",
      "\n",
      "--- EOS Token ID ---\n",
      "50256\n",
      "\n",
      "--- PAD Token ID (설정되었다면 EOS와 동일) ---\n",
      "50256\n",
      "\n",
      "--- 최종 데이터 구조 샘플 (첫 번째) ---\n",
      "{\n",
      "  \"input_text\": \"<DUE>2021-05-13 <ITEM>K04033 <COST>25870 <QTY>318 <URGENT>1\",\n",
      "  \"conditioning_vector\": {\n",
      "    \"K04033\": {\n",
      "      \"404\": 12.84,\n",
      "      \"405\": 10.42,\n",
      "      \"407\": 8.69,\n",
      "      \"408\": 9.12,\n",
      "      \"409\": 10.33,\n",
      "      \"410\": 8.99,\n",
      "      \"412\": 12.32,\n",
      "      \"416\": 11.23,\n",
      "      \"422\": 4.0,\n",
      "      \"424\": 9.87,\n",
      "      \"426\": 11.77,\n",
      "      \"433\": 9.54,\n",
      "      \"434\": 13.05,\n",
      "      \"435\": 9.0,\n",
      "      \"436\": 9.0,\n",
      "      \"438\": 4.4,\n",
      "      \"439\": 12.47,\n",
      "      \"440\": 9.4\n",
      "    }\n",
      "  },\n",
      "  \"target_vector\": {\n",
      "    \"output_text\": \"더미 GT 결과: 생산 계획 최적화 완료.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 셀 4: input_text 문자열 생성 및 토큰화 확인\n",
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    "import json # JSON 형태의 출력을 위해 임포트 (선택 사항)\n",
    "\n",
    "# --- GPT-2 토크나이저 로드 ---\n",
    "try:\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "    # GPT-2 토크나이저는 기본적으로 pad_token이 없으므로, eos_token을 pad_token으로 설정 (필요시)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"GPT-2 토크나이저 로드 완료.\")\n",
    "except Exception as e:\n",
    "    print(f\"토크나이저 로드 중 오류 발생: {e}\")\n",
    "    tokenizer = None # 오류 발생 시 None으로 설정\n",
    "\n",
    "# --- input_text 문자열 생성 및 최종 데이터 구조 업데이트 ---\n",
    "final_processed_data_list = []\n",
    "\n",
    "if 'processed_data_list' in globals() and tokenizer: # processed_data_list가 있고 토크나이저가 로드되었는지 확인\n",
    "    for data_sample in processed_data_list:\n",
    "        components = data_sample['input_text_components']\n",
    "        \n",
    "        # input_text_components 딕셔너리의 순서를 유지하며 문자열 생성\n",
    "        # Python 3.7+ 에서는 딕셔너리 삽입 순서가 유지되지만, 명시적으로 순서를 정의하는 것이 더 안전할 수 있음\n",
    "        # 여기서는 딕셔너리 생성 시의 순서(<DUE>, <ITEM>, <COST>, <QTY>, <URGENT>)를 따른다고 가정합니다.\n",
    "        input_text_str = \" \".join([f\"{tag}{value}\" for tag, value in components.items()])\n",
    "        \n",
    "        # 기존 data_sample 딕셔너리를 복사하고 input_text_str 추가, input_text_components는 제거 (선택)\n",
    "        final_sample = {\n",
    "            \"input_text\": input_text_str,\n",
    "            \"conditioning_vector\": data_sample['conditioning_vector'],\n",
    "            \"target_vector\": data_sample['target_vector']\n",
    "        }\n",
    "        final_processed_data_list.append(final_sample)\n",
    "\n",
    "    print(f\"\\n총 {len(final_processed_data_list)}개의 최종 데이터 샘플 생성 완료.\")\n",
    "\n",
    "    # --- 토큰화 결과 확인 (첫 번째 샘플 데이터 대상) ---\n",
    "    if len(final_processed_data_list) > 0:\n",
    "        sample_for_tokenization = final_processed_data_list[0]\n",
    "        text_to_tokenize = sample_for_tokenization['input_text']\n",
    "        \n",
    "        print(f\"\\n--- 토큰화할 텍스트 샘플 ---\")\n",
    "        print(text_to_tokenize)\n",
    "        \n",
    "        # 토큰화 수행\n",
    "        # add_special_tokens=True를 사용하면 문장 시작/끝에 특수 토큰이 추가될 수 있습니다.\n",
    "        # 여기서는 False로 하여 순수 텍스트에 대한 토큰화 결과를 봅니다.\n",
    "        tokenized_output = tokenizer(text_to_tokenize, add_special_tokens=False)\n",
    "        input_ids = tokenized_output['input_ids']\n",
    "        decoded_tokens = [tokenizer.decode([token_id]) for token_id in input_ids]\n",
    "        \n",
    "        print(f\"\\n--- 토큰화 결과 (Token IDs) ---\")\n",
    "        print(input_ids)\n",
    "        \n",
    "        print(f\"\\n--- 디코딩된 각 토큰 ---\")\n",
    "        print(decoded_tokens)\n",
    "        \n",
    "        print(f\"\\n--- 토크나이저 어휘 크기 (Vocabulary Size) ---\")\n",
    "        print(tokenizer.vocab_size)\n",
    "        \n",
    "        print(f\"\\n--- EOS Token ID ---\")\n",
    "        print(tokenizer.eos_token_id)\n",
    "\n",
    "        print(f\"\\n--- PAD Token ID (설정되었다면 EOS와 동일) ---\")\n",
    "        print(tokenizer.pad_token_id)\n",
    "\n",
    "        # 전체 최종 데이터 구조 중 첫 번째 샘플 출력 (JSON 유사 형태)\n",
    "        print(\"\\n--- 최종 데이터 구조 샘플 (첫 번째) ---\")\n",
    "        # json.dumps를 사용하면 딕셔너리를 예쁘게 출력할 수 있습니다.\n",
    "        print(json.dumps(final_processed_data_list[0], indent=2, ensure_ascii=False))\n",
    "        \n",
    "    else:\n",
    "        print(\"토큰화할 데이터가 없습니다 (final_processed_data_list가 비어있음).\")\n",
    "elif not 'processed_data_list' in globals():\n",
    "    print(\"오류: 이전 단계에서 'processed_data_list'가 생성되지 않았습니다.\")\n",
    "elif not tokenizer:\n",
    "    print(\"오류: 토크나이저가 성공적으로 로드되지 않아 토큰화를 진행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64034120-a9a7-4854-b0ce-cf94f672fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "더미 데이터셋 및 데이터로더 생성 완료.\n",
      "Input tensor shape: torch.Size([127, 128])\n",
      "Target tensor shape: torch.Size([127, 128])\n",
      "옵티마이저 설정 완료: <class 'torch.optim.adamw.AdamW'>\n",
      "SEDD 손실 함수 준비 완료.\n",
      "\n",
      "더미 학습 루프 시작...\n",
      "Epoch 1/1, Batch 1/64, Loss: 1063.8622\n",
      "Epoch 1/1, Batch 2/64, Loss: 962.2855\n",
      "Epoch 1/1, Batch 3/64, Loss: 906.1330\n",
      "Epoch 1/1, Batch 4/64, Loss: 940.7695\n",
      "Epoch 1/1, Batch 5/64, Loss: 533.3435\n",
      "Epoch 1/1, Batch 6/64, Loss: 598.0334\n",
      "Epoch 1/1, Batch 7/64, Loss: 593.1202\n",
      "Epoch 1/1, Batch 8/64, Loss: 618.6344\n",
      "Epoch 1/1, Batch 9/64, Loss: 423.3824\n",
      "Epoch 1/1, Batch 10/64, Loss: 529.9001\n",
      "Epoch 1/1, Batch 11/64, Loss: 398.9142\n",
      "Epoch 1/1, Batch 12/64, Loss: 456.2115\n",
      "Epoch 1/1, Batch 13/64, Loss: 230.7086\n",
      "Epoch 1/1, Batch 14/64, Loss: 416.3353\n",
      "Epoch 1/1, Batch 15/64, Loss: 303.1117\n",
      "Epoch 1/1, Batch 16/64, Loss: 595.3788\n",
      "Epoch 1/1, Batch 17/64, Loss: 368.9520\n",
      "Epoch 1/1, Batch 18/64, Loss: 296.6279\n",
      "Epoch 1/1, Batch 19/64, Loss: 246.1158\n",
      "Epoch 1/1, Batch 20/64, Loss: 584.5085\n",
      "Epoch 1/1, Batch 21/64, Loss: 260.0400\n",
      "Epoch 1/1, Batch 22/64, Loss: 165.8648\n",
      "Epoch 1/1, Batch 23/64, Loss: 192.4518\n",
      "Epoch 1/1, Batch 24/64, Loss: 195.3475\n",
      "Epoch 1/1, Batch 25/64, Loss: 299.8276\n",
      "Epoch 1/1, Batch 26/64, Loss: 376.9040\n",
      "Epoch 1/1, Batch 27/64, Loss: 232.9868\n",
      "Epoch 1/1, Batch 28/64, Loss: 143.5471\n",
      "Epoch 1/1, Batch 29/64, Loss: 129.0068\n",
      "Epoch 1/1, Batch 30/64, Loss: 144.4629\n",
      "Epoch 1/1, Batch 31/64, Loss: 194.3216\n",
      "Epoch 1/1, Batch 32/64, Loss: 468.1652\n",
      "Epoch 1/1, Batch 33/64, Loss: 179.1277\n",
      "Epoch 1/1, Batch 34/64, Loss: 243.6218\n",
      "Epoch 1/1, Batch 35/64, Loss: 190.9336\n",
      "Epoch 1/1, Batch 36/64, Loss: 259.0622\n",
      "Epoch 1/1, Batch 37/64, Loss: 200.8175\n",
      "Epoch 1/1, Batch 38/64, Loss: 124.1009\n",
      "Epoch 1/1, Batch 39/64, Loss: 66.6242\n",
      "Epoch 1/1, Batch 40/64, Loss: 252.9322\n",
      "Epoch 1/1, Batch 41/64, Loss: 168.7689\n",
      "Epoch 1/1, Batch 42/64, Loss: 144.6623\n",
      "Epoch 1/1, Batch 43/64, Loss: 96.9970\n",
      "Epoch 1/1, Batch 44/64, Loss: 380.2595\n",
      "Epoch 1/1, Batch 45/64, Loss: 139.4551\n",
      "Epoch 1/1, Batch 46/64, Loss: 209.4929\n",
      "Epoch 1/1, Batch 47/64, Loss: 369.5355\n",
      "Epoch 1/1, Batch 48/64, Loss: 163.1473\n",
      "Epoch 1/1, Batch 49/64, Loss: 246.8512\n",
      "Epoch 1/1, Batch 50/64, Loss: 164.5359\n",
      "Epoch 1/1, Batch 51/64, Loss: 251.1240\n",
      "Epoch 1/1, Batch 52/64, Loss: 143.6331\n",
      "Epoch 1/1, Batch 53/64, Loss: 90.2149\n",
      "Epoch 1/1, Batch 54/64, Loss: 154.5267\n",
      "Epoch 1/1, Batch 55/64, Loss: 162.6559\n",
      "Epoch 1/1, Batch 56/64, Loss: 114.5372\n",
      "Epoch 1/1, Batch 57/64, Loss: 81.0819\n",
      "Epoch 1/1, Batch 58/64, Loss: 129.0892\n",
      "Epoch 1/1, Batch 59/64, Loss: 459.6176\n",
      "Epoch 1/1, Batch 60/64, Loss: 339.5596\n",
      "Epoch 1/1, Batch 61/64, Loss: 295.3949\n",
      "Epoch 1/1, Batch 62/64, Loss: 170.8701\n",
      "Epoch 1/1, Batch 63/64, Loss: 214.1760\n",
      "Epoch 1/1, Batch 64/64, Loss: 227.6166\n",
      "Epoch 1 완료. 평균 Loss: 317.2543\n",
      "더미 학습 루프 종료.\n"
     ]
    }
   ],
   "source": [
    "# 셀 5: 더미 GT 생성 및 SEDD 모델 파인튜닝 흐름 확인\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# SEDD 레포지토리의 유틸리티 함수 임포트\n",
    "# 경로가 올바르게 설정되어 있어야 합니다. (셀 1에서 처리됨)\n",
    "import losses # losses.py\n",
    "from model import utils as mutils # model/utils.py\n",
    "# import graph_lib # 이미 로드됨 (graph 변수 사용)\n",
    "# import noise_lib # 이미 로드됨 (noise 변수 사용)\n",
    "\n",
    "# --- 하이퍼파라미터 (매우 간소화된 버전) ---\n",
    "dummy_learning_rate = 1e-5\n",
    "dummy_num_epochs = 1 # 실제 학습이 아니므로 1 epoch만\n",
    "dummy_batch_size = 2 # 작은 배치 크기로 테스트\n",
    "max_seq_length = 128 # 모델이 처리할 수 있는 최대 시퀀스 길이 (필요시 SEDD 설정 확인 후 조정)\n",
    "                     # GPT-2는 1024까지 가능하지만, 더미 학습이므로 짧게 설정하여 메모리/시간 절약\n",
    "\n",
    "# --- 데이터셋 및 데이터로더 준비 (더미 학습용) ---\n",
    "# final_processed_data_list를 사용합니다.\n",
    "# 각 \"input_text\"를 토큰화하고, \"target_vector\"의 \"output_text\"도 토큰화합니다.\n",
    "\n",
    "# 1. 입력 텍스트 토큰화\n",
    "input_sequences = []\n",
    "for sample in final_processed_data_list:\n",
    "    # conditioning_vector는 SEDD 모델 직접 입력이 아니므로 여기서는 제외\n",
    "    # 실제로는 모델 아키텍처에 따라 conditioning_vector를 사용하는 방식이 있을 수 있음\n",
    "    tokenized_input = tokenizer(sample['input_text'], \n",
    "                                padding='max_length',      # 최대 길이에 맞춰 패딩\n",
    "                                truncation=True,           # 최대 길이 초과시 자르기\n",
    "                                max_length=max_seq_length, \n",
    "                                return_tensors=\"pt\")       # PyTorch 텐서로 반환\n",
    "    input_sequences.append(tokenized_input['input_ids'].squeeze(0)) # 배치 차원 제거\n",
    "\n",
    "# 2. 타겟 텍스트(더미 GT) 토큰화\n",
    "# 모든 샘플에 대해 동일한 더미 타겟을 사용하거나, 입력과 관련된 간단한 타겟을 만들 수 있습니다.\n",
    "# 여기서는 \"input_text\"의 처음 몇 토큰을 반복하는 것으로 가정 (매우 단순한 예시)\n",
    "# 또는 고정된 더미 텍스트 사용\n",
    "dummy_gt_text = \"더미 GT 결과입니다.\" # 모든 샘플에 동일한 GT 사용\n",
    "tokenized_dummy_gt = tokenizer(dummy_gt_text,\n",
    "                               padding='max_length',\n",
    "                               truncation=True,\n",
    "                               max_length=max_seq_length,\n",
    "                               return_tensors=\"pt\")['input_ids'].squeeze(0)\n",
    "\n",
    "target_sequences = [tokenized_dummy_gt.clone() for _ in range(len(input_sequences))]\n",
    "\n",
    "\n",
    "# PyTorch Dataset 및 DataLoader 생성\n",
    "if input_sequences and target_sequences:\n",
    "    input_tensor = torch.stack(input_sequences)\n",
    "    target_tensor = torch.stack(target_sequences)\n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=dummy_batch_size, shuffle=True)\n",
    "    print(f\"\\n더미 데이터셋 및 데이터로더 생성 완료.\")\n",
    "    print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "    print(f\"Target tensor shape: {target_tensor.shape}\")\n",
    "else:\n",
    "    print(\"\\n오류: 입력 또는 타겟 시퀀스가 비어있어 데이터로더를 생성할 수 없습니다.\")\n",
    "    dataloader = None\n",
    "\n",
    "# --- 모델, 옵티마이저, 손실 함수 등 설정 ---\n",
    "if dataloader:\n",
    "    # 모델을 학습 모드로 설정\n",
    "    score_model.train()\n",
    "\n",
    "    # 옵티마이저 설정 (SEDD 레포의 losses.get_optimizer 사용 가능)\n",
    "    # 간소화를 위해 AdamW 직접 사용\n",
    "    # 실제 SEDD 학습에는 config 객체가 필요하므로, 여기서는 단순화합니다.\n",
    "    optimizer = optim.AdamW(score_model.parameters(), lr=dummy_learning_rate)\n",
    "    print(f\"옵티마이저 설정 완료: {type(optimizer)}\")\n",
    "\n",
    "    # 손실 함수 설정 (SEDD 레포의 losses.get_loss_fn 사용)\n",
    "    # get_loss_fn에는 noise, graph, train 인자가 필요합니다.\n",
    "    # sampling_eps 등 추가 파라미터는 기본값 사용 가능\n",
    "    # 이 loss_fn은 (model, batch, cond, t, perturbed_batch) 등을 인자로 받음\n",
    "    # 현재 우리는 조건(cond), 시간(t), perturbed_batch를 명시적으로 제어하지 않으므로,\n",
    "    # SEDD의 학습 방식에 맞게 batch (토큰 ID 시퀀스)만 전달하는 형태로\n",
    "    # 실제 loss 계산이 이루어지는지 확인해야 합니다.\n",
    "    # SEDD의 loss_fn은 내부적으로 noise 샘플링, 교란 등을 수행합니다.\n",
    "    \n",
    "    # SEDD의 loss_fn 시그니처에 맞게 더미 학습 루프 조정 필요\n",
    "    # loss_fn_sedd = losses.get_loss_fn(noise.to(device), graph, train=True)\n",
    "    # 위 noise.to(device)는 이미 device에 있으므로 중복일 수 있습니다.\n",
    "    # load_model에서 반환된 noise, graph 객체를 사용합니다.\n",
    "    loss_fn_sedd = losses.get_loss_fn(noise, graph, train=True)\n",
    "    print(f\"SEDD 손실 함수 준비 완료.\")\n",
    "\n",
    "    # --- 더미 학습 루프 ---\n",
    "    print(\"\\n더미 학습 루프 시작...\")\n",
    "    for epoch in range(dummy_num_epochs):\n",
    "        total_loss_epoch = 0\n",
    "        for batch_idx, (input_batch, target_batch) in enumerate(dataloader):\n",
    "            input_batch = input_batch.to(device)\n",
    "            # target_batch는 SEDD의 score_entropy loss 계산 시 x0 (원본 데이터)로 사용될 수 있습니다.\n",
    "            # SEDD의 loss_fn은 (model, batch_x0, cond=None, t=None, perturbed_batch=None) 형태로 호출,\n",
    "            # 여기서 batch_x0가 원본 데이터를 의미합니다.\n",
    "            # 그리고 이 batch_x0로부터 내부적으로 perturbed_batch를 생성합니다.\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # SEDD의 get_loss_fn으로 생성된 loss_fn은 (model, batch)를 받습니다.\n",
    "            # 이 batch가 x_0 (원본 깨끗한 데이터)에 해당합니다.\n",
    "            # loss_fn 내부에서 t 샘플링, 노이즈 추가, 모델 호출, score_entropy 계산 등이 이루어집니다.\n",
    "            try:\n",
    "                # score_model은 DDP로 감싸져 있을 수 있으나, loss_fn_sedd 내부에서 처리될 것으로 기대\n",
    "                loss = loss_fn_sedd(score_model, input_batch) \n",
    "                \n",
    "                # loss가 텐서 리스트 등으로 반환될 경우 .mean() 등이 필요할 수 있음\n",
    "                if isinstance(loss, list) or (isinstance(loss, torch.Tensor) and loss.ndim > 0 and loss.numel() > 1) :\n",
    "                    loss = loss.mean() \n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss_epoch += loss.item()\n",
    "                \n",
    "                if batch_idx % 1 == 0: # 모든 배치마다 로그 (더미이므로)\n",
    "                    print(f\"Epoch {epoch+1}/{dummy_num_epochs}, Batch {batch_idx+1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "                \n",
    "                # 아주 기본적인 확인: gradient가 흐르는지 (옵션)\n",
    "                # if batch_idx == 0:\n",
    "                #     for name, param in score_model.named_parameters():\n",
    "                #         if param.grad is not None:\n",
    "                #             print(f\"Gradient found for: {name}, grad norm: {param.grad.norm().item()}\")\n",
    "                #         else:\n",
    "                #             print(f\"No gradient for: {name}\")\n",
    "                #         if 'vocab_embed' in name: # 너무 많으니 일부만\n",
    "                #             break\n",
    "                            \n",
    "            except Exception as e_train:\n",
    "                print(f\"학습 중 오류 발생 (Epoch {epoch+1}, Batch {batch_idx+1}): {e_train}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                break # 오류 발생 시 해당 에폭 중단\n",
    "        \n",
    "        avg_loss_epoch = total_loss_epoch / len(dataloader) if len(dataloader) > 0 else 0\n",
    "        print(f\"Epoch {epoch+1} 완료. 평균 Loss: {avg_loss_epoch:.4f}\")\n",
    "        if 'e_train' in locals() and e_train: break # 학습 중 오류 시 전체 루프 중단\n",
    "\n",
    "    print(\"더미 학습 루프 종료.\")\n",
    "else:\n",
    "    print(\"데이터로더가 생성되지 않아 학습 루프를 실행할 수 없습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sedd_env)",
   "language": "python",
   "name": "sedd_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
